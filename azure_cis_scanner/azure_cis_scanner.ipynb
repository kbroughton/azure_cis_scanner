{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widen the display to fit your screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run it all from the commandline (in the docker container) with the following:\n",
    "base_dir is where the output scans directory will go.  -t 500 is so that it doesn't timeout.\n",
    "run_jnb -a '{\"subscription_id\": \"510f92e0-xxxx-yyyy-zzzz-095d37e6a299\", \"base_dir\": \"/engagements/cis_test\"}' -v  azure_cis_scanner.ipynb -t 500\n",
    "\n",
    "to run across multiple subscription_id's do\n",
    "for subscription in `cat /engagements/cis_test/scans/accounts.json | jq '.[].id'`; \\\n",
    "   do run_jnb -a '{\"subscription_id\": $subscription, \"base_dir\": \"/engagements/cis_test\"}' -v  azure_cis_scanner.ipynb -t 500; \\\n",
    "   done\n",
    "\n",
    "For more on run_jnb see https://github.com/hz-inova/run_jnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-common==1.1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jmespath azurerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jmespath\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that you have azure cli installed\n",
    "!az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if not, follow the instructions to install\n",
    "https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that yor are connected to the correct account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to azure.  This may require doing from the laptop/vm commandline or exec into the container\n",
    "#!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your subscription_id here or run from the commandline\n",
    "#subscription_id = 'aaaaaaaa-bbbbb-4cccc-dddd-eeeeeeeee0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!az account set --subscription {subscription_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = !az account show\n",
    "account = yaml.load(account.nlstr)\n",
    "account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = account['id']\n",
    "subscription_name = account['name']\n",
    "subscription_dirname = subscription_name.split(' ')[0] + '-' + subscription_id.split('-')[0]\n",
    "subscription_dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/engagements/cis_test/'\n",
    "scanner_dir = '/praetorian-tools/azure_cis_scanner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /praetorian-tools/azure_cis_scanner/scanner/utils.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write to disk or load as needed with %%writefile or %load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile /praetorian-tools/azure_cis_scanner/scanner/utils.py\n",
    "# %load /praetorian-tools/azure_cis_scanner/scanner/utils.py\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import functools\n",
    "import json\n",
    "import requests\n",
    "\n",
    "token_expiry = None\n",
    "access_token = None\n",
    "filtered_data_dir = ''\n",
    "scan_data_dir = ''\n",
    "raw_data_dir = ''\n",
    "\n",
    "def set_data_paths(subscription_dirname, base_dir='.'):\n",
    "    \"\"\"\n",
    "    Given a base_dir, create subdirs scans/{day}/raw\n",
    "                                          /filtered\n",
    "    @returns: scan_data_dir, raw_data_dir\n",
    "    \"\"\"\n",
    "    # Get day in YYYY-MM-DD format\n",
    "\n",
    "    day = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    scan_data_dir = os.path.join(base_dir, 'scans', subscription_dirname, day)\n",
    "    print(\"scan_data_dir\", scan_data_dir)\n",
    "    raw_data_dir = scan_data_dir + '/raw'\n",
    "    print(\"raw_data_dir\", raw_data_dir)\n",
    "    if not os.path.exists(raw_data_dir):\n",
    "        os.makedirs(raw_data_dir)\n",
    "    filtered_data_dir = scan_data_dir + '/filtered'\n",
    "    print(\"filtered_data_dir\", filtered_data_dir)\n",
    "    if not os.path.exists(filtered_data_dir):\n",
    "        os.makedirs(filtered_data_dir)\n",
    "    return scan_data_dir, raw_data_dir, filtered_data_dir   \n",
    "\n",
    "def call(command, retrieving_access_token=False, stderr=subprocess.STDOUT):\n",
    "    if not valid_token() and not retrieving_access_token:\n",
    "        get_access_token()\n",
    "    if(isinstance(command, str)) :\n",
    "        command = command.split()           # subprocess needs an array of arguments\n",
    "    try :\n",
    "        print('running: ', command)\n",
    "        return subprocess.check_output(command, shell=False, stderr=stderr).decode('utf-8')\n",
    "    except:\n",
    "        print(\"An exception occurred while processing command \" + str(command) + \" Halting execution!\")\n",
    "        sys.exit()\n",
    "\n",
    "def verify_subscription_id_format(subscriptionId) :\n",
    "    r = re.compile(\"([a-f]|[0-9]){8}-([a-f]|[0-9]){4}-([a-f]|[0-9]){4}-([a-f]|[0-9]){4}-([a-f]|[0-9]){12}\")\n",
    "    if r.match(subscriptionId):\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def valid_token():\n",
    "     if (not token_expiry) or (datetime.datetime.utcnow() > token_expiry):\n",
    "        return False\n",
    "     else:\n",
    "        return True\n",
    "    \n",
    "def get_subscription_id() :\n",
    "    current_context = jsonify(call(\"az account show\"))\n",
    "    return current_context[\"id\"]\n",
    "\n",
    "def get_access_token():\n",
    "    global token_expiry, access_token\n",
    "    if not valid_token():\n",
    "        complete_token = call(\"az account get-access-token\", retrieving_access_token=True)\n",
    "        complete_token = jsonify(complete_token)\n",
    "        access_token = complete_token[\"accessToken\"]\n",
    "        token_expiry = complete_token[\"expiresOn\"]\n",
    "        print(token_expiry)\n",
    "        token_expiry = datetime.datetime.strptime(token_expiry, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    return access_token\n",
    "\n",
    "def make_request(url, args=[]):\n",
    "    print('requesting ', url)\n",
    "    authorization_headers = {\"Authorization\" : \"Bearer \" + get_access_token()}\n",
    "    r = requests.get(url, headers=authorization_headers)\n",
    "    return r.text\n",
    "\n",
    "def jsonify(jsonString):\n",
    "    print(jsonString)\n",
    "    return json.loads(jsonString)\n",
    "\n",
    "def stringify(jsonObject) :\n",
    "    return json.dumps(jsonObject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1,2]).difference(set([1,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_access_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run set_data_paths to create {base_dir}/phases/azure/scans/{date}/raw json files for raw output of commands\n",
    "Run the scripts for the selection criteria that determine failure to produce a filtered subset of the above writing to /phases/azure/scans/{date}/granular_findings_json for the json data of a finding\n",
    "Run a script to generate the grouped findings at least partially\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_expiry)\n",
    "print(datetime.datetime.utcnow())\n",
    "print(datetime.datetime.utcnow() > token_expiry)\n",
    "valid_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are running this notebook from a container with ~/engagements mounted into /engagements in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_data_dir, raw_data_dir, filtered_data_dir = set_data_paths(subscription_dirname, base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-ride dates if necessary\n",
    "# scan_data_dir = '/engagements/cis_test/scans/2018-05-26'\n",
    "# raw_data_dir = '/engagements/cis_test/scans/2018-05-26/raw'\n",
    "# filtered_data_dir = '/engagements/cis_test/scans/2018-05-26/filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error like the following indicates you have not logged in correctly\n",
    "ScannerError: while scanning a simple key\n",
    "  in \"<unicode string>\", line 6, column 1:\n",
    "    100    97  100    97    0     0  ... \n",
    "    ^\n",
    "could not find expected ':'\n",
    "  in \"<unicode string>\", line 7, column 1:\n",
    "    {\"error\":{\"code\":\"SubscriptionNo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile /praetorian-tools/azure_cis_scanner/scanner/utils.py\n",
    "# %load /praetorian-tools/azure_cis_scanner/scanner/utils.py\n",
    "\n",
    "accounts_path = os.path.join(base_dir, 'scans', 'accounts.json')\n",
    "\n",
    "def get_accounts(accounts_path):\n",
    "    \"\"\"\n",
    "    @accounts_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    accounts = !az account list\n",
    "    accounts = yaml.safe_load(accounts.nlstr)\n",
    "    with open(accounts_path, 'w') as f:\n",
    "        json.dump(accounts, f, indent=4, sort_keys=True)\n",
    "    return accounts\n",
    "\n",
    "def load_accounts(accounts_path):\n",
    "    with open(accounts_path, 'r') as f:\n",
    "        accounts = yaml.safe_load(f)\n",
    "    return accounts\n",
    "\n",
    "def get_resource_groups(resource_groups_path):\n",
    "    \"\"\"\n",
    "    @network_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    resource_groups = !az group list\n",
    "    resource_groups = yaml.safe_load(resource_groups.nlstr)\n",
    "    with open(resource_groups_path, 'w') as f:\n",
    "        json.dump(resource_groups, f, indent=4, sort_keys=True)\n",
    "    return resource_groups\n",
    "\n",
    "def load_resource_groups(resource_groups_path):\n",
    "    with open(resource_groups_path, 'r') as f:\n",
    "        resource_groups = yaml.safe_load(f)\n",
    "    return resource_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accounts_path)\n",
    "accounts = get_accounts(accounts_path)\n",
    "print(accounts)\n",
    "resource_groups_path = os.path.join(raw_data_dir, \"resource_groups.json\")\n",
    "get_resource_groups(resource_groups_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scanner_dir}/scanner/modules/security_center.py\n",
    "#%load {scanner_dir}/scanner/modules/security_center.py\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "security_center_path = os.path.join(raw_data_dir, \"security_center.json\")\n",
    "security_center_filtered_path = os.path.join(filtered_data_dir, 'security_center_filtered.json')\n",
    "\n",
    "def get_security_center(security_center_path):\n",
    "    \"\"\"\n",
    "    Query Azure api for storage accounts info and save to disk\n",
    "    \"\"\"\n",
    "    output = !az account get-access-token --query \"{subscripton:subscription,accessToken:accessToken}\" --out tsv\n",
    "    print(output.nlstr.split())\n",
    "    subscription_id, token = output.nlstr.split()\n",
    "    security_center = !curl -X GET -H \"Authorization: Bearer {token}\" -H \"Content-Type: application/json\" https://management.azure.com/subscriptions/{subscription_id}/providers/microsoft.Security/policies?api-version=2015-06-01-preview 2>/dev/null\n",
    "    security_center = yaml.load(security_center.nlstr)\n",
    "    security_center = security_center['value']\n",
    "    print(security_center)\n",
    "        \n",
    "    with open(security_center_path, 'w') as f:\n",
    "        yaml.dump(security_center, f)\n",
    "    return security_center\n",
    "\n",
    "def load_security_center(security_center_filtered_path):\n",
    "    with open(security_center_path, 'r') as f:\n",
    "        security_center = yaml.load(f)\n",
    "    return security_center\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Generate json for the security_center findings\n",
    "    \"\"\"\n",
    "    get_security_center(security_center_path)\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Generate filtered (failing) output in json\n",
    "    \"\"\"\n",
    "    security_center = load_security_center(security_center_path)\n",
    "    security_center_results = {}\n",
    "\n",
    "    security_center_results['automatic_provisioning_of_monitoring_agent_is_set_to_on'] = automatic_provisioning_of_monitoring_agent_is_set_to_on_2_2(security_center)\n",
    "    security_center_results['system_updates_is_set_to_on'] = system_updates_is_set_to_on_2_3(security_center)\n",
    "    security_center_results['security_configurations_is_set_to_on'] = security_configurations_is_set_to_on_2_4(security_center)\n",
    "    security_center_results['endpoint_protection_is_set_to_on'] = endpoint_protection_is_set_to_on_2_5(security_center)\n",
    "    security_center_results['disk_encryption_is_set_to_on'] = disk_encryption_is_set_to_on_2_6(security_center)\n",
    "    security_center_results['network_security_groups_is_set_to_on'] = network_security_groups_is_set_to_on_2_7(security_center)\n",
    "    security_center_results['web_application_firewall_is_set_to_on'] = web_application_firewall_is_set_to_on_2_8(security_center)\n",
    "    security_center_results['next_generation_firewall_is_set_to_on'] = next_generation_firewall_is_set_to_on_2_9(security_center)\n",
    "    security_center_results['vulnerability_assessment_is_set_to_on'] = vulnerability_assessment_is_set_to_on_2_10(security_center)\n",
    "    security_center_results['storage_encryption_is_set_to_on'] = storage_encryption_is_set_to_on_2_11(security_center)\n",
    "    security_center_results['just_in_time_access_is_set_to_on'] = just_in_time_access_is_set_to_on_2_12(security_center)\n",
    "    security_center_results['adaptive_application_controls_is_set_to_on'] = adaptive_application_controls_is_set_to_on_2_13(security_center)\n",
    "    security_center_results['sql_auditing_and_threat_detection_is_set_to_on'] = sql_auditing_and_threat_detection_is_set_to_on_2_14(security_center)\n",
    "    security_center_results['sql_encryption_is_set_to_on'] = sql_encryption_is_set_to_on_2_15(security_center)\n",
    "    security_center_results['security_contact_emails_is_set'] = security_contact_emails_is_set_2_16(security_center)\n",
    "    security_center_results['security_contact_phone_number_is_set'] = security_contact_phone_number_is_set_2_17(security_center)\n",
    "    security_center_results['send_me_emails_about_alerts_is_set_to_on'] = send_me_emails_about_alerts_is_set_to_on_2_18(security_center)\n",
    "    security_center_results['send_email_also_to_subscription_owners_is_set_to_on'] = send_email_also_to_subscription_owners_is_set_to_on_2_19(security_center)\n",
    "                \n",
    "    with open(security_center_filtered_path, 'w') as f:\n",
    "        json.dump(security_center_results, f, indent=4, sort_keys=True)\n",
    "    return security_center_results\n",
    "\n",
    "def automatic_provisioning_of_monitoring_agent_is_set_to_on_2_2(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        automatic_provisioning_of_monitoring_agent = item['properties']['logCollection']\n",
    "        if automatic_provisioning_of_monitoring_agent != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"automatic_provisioning_of_monitoring_agent_is_set_to_on\",\n",
    "                \"negative_name\": \"automatic_provisioning_of_monitoring_agent_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "    \n",
    "def system_updates_is_set_to_on_2_3(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        system_updates = item['properties']['recommendations']['patch']\n",
    "        if system_updates != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"system_updates_is_set_to_on\",\n",
    "                \"negative_name\": \"system_updates_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}    \n",
    "\n",
    "def security_configurations_is_set_to_on_2_4(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        security_configurations = item['properties']['recommendations']['baseline']\n",
    "        if security_configurations != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"security_configurations_is_set_to_on\",\n",
    "                \"negative_name\": \"security_configurations_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata} \n",
    "\n",
    "def endpoint_protection_is_set_to_on_2_5(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        endpoint_protection = item['properties']['recommendations']['antimalware']\n",
    "        if endpoint_protection != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"endpoint_protection_is_set_to_on\",\n",
    "                \"negative_name\": \"endpoint_protection_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def disk_encryption_is_set_to_on_2_6(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        disk_encryption = item['properties']['recommendations']['diskEncryption']\n",
    "        if disk_encryption != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"disk_encryption_is_set_to_on\",\n",
    "                \"negative_name\": \"disk_encryption_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def network_security_groups_is_set_to_on_2_7(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        nsgs = item['properties']['recommendations']['nsgs']\n",
    "        if nsgs != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"network_security_groups_is_set_to_on\",\n",
    "                \"negative_name\": \"network_security_groups_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def web_application_firewall_is_set_to_on_2_8(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        waf = item['properties']['recommendations']['waf']\n",
    "        if waf != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"web_application_firewall_is_set_to_on\",\n",
    "                \"negative_name\": \"web_application_firewall_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def next_generation_firewall_is_set_to_on_2_9(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        ngfw = item['properties']['recommendations']['ngfw']\n",
    "        if ngfw != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"automatic_provisioning_of_monitoring_agent_is_set_to_on\",\n",
    "                \"negative_name\": \"automatic_provisioning_of_monitoring_agent_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def vulnerability_assessment_is_set_to_on_2_10(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        vulnerability_assessment = item['properties']['recommendations']['vulnerabilityAssessment']\n",
    "        if vulnerability_assessment != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"automatic_provisioning_of_monitoring_agent_is_set_to_on\",\n",
    "                \"negative_name\": \"automatic_provisioning_of_monitoring_agent_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def storage_encryption_is_set_to_on_2_11(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        storage_encryption = item['properties']['recommendations']['storageEncryption']\n",
    "        if storage_encryption != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"storage_encryption_is_set_to_on\",\n",
    "                \"negative_name\": \"storage_encryption_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def just_in_time_access_is_set_to_on_2_12(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        jit = item['properties']['recommendations']['jitNetworkAccess']\n",
    "        if jit != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"just_in_time_access_is_set_to_on\",\n",
    "                \"negative_name\": \"just_in_time_access_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def adaptive_application_controls_is_set_to_on_2_13(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        security_configurations = item['properties']['recommendations']['appWhitelisting']\n",
    "        if security_configurations != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"adaptive_application_controls_is_set_to_on\",\n",
    "                \"negative_name\": \"adaptive_application_controls_noto_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def sql_auditing_and_threat_detection_is_set_to_on_2_14(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        sqlAuditing = item['properties']['recommendations']['sqlAuditing']\n",
    "        if sqlAuditing != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"sql_auditing_and_threat_detection_is_set_to_on\",\n",
    "                \"negative_name\": \"sql_auditing_and_threat_detection_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def sql_encryption_is_set_to_on_2_15(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        sql_tde = item['properties']['recommendations']['sqlTde']\n",
    "        if sql_tde != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"sql_encryption_is_set_to_on\",\n",
    "                \"negative_name\": \"sql_encryption_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def security_contact_emails_is_set_2_16(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        emails = item['properties']['securityContactConfiguration']['securityContactEmails']\n",
    "        if not emails:\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"security_contact_emails_is_set\",\n",
    "                \"negative_name\": \"security_contact_emails_not_set\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def security_contact_phone_number_is_set_2_17(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        phone = item['properties']['securityContactConfiguration']['securityContactPhone']\n",
    "        if not phone:\n",
    "            items_flagged_list.append((resource_group))\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"security_contact_phone_number_is_set\",\n",
    "                \"negative_name\": \"security_contact_phone_number_not_set\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def send_me_emails_about_alerts_is_set_to_on_2_18(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        notifications = item['properties']['securityContactConfiguration']['areNotificationsOn']\n",
    "        if notifications != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"send_email_alerts_about_alerts_is_set_to_on\",\n",
    "                \"negative_name\": \"send_email_alerts_about_alerts_not_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def send_email_also_to_subscription_owners_is_set_to_on_2_19(security_center):\n",
    "    items_flagged_list = []\n",
    "    for item in security_center:\n",
    "        resource_group = item['name']\n",
    "        send_admin = item['properties']['securityContactConfiguration']['sendToAdminOn']\n",
    "        if send_admin != \"On\":\n",
    "            items_flagged_list.append((resource_group))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(security_center)}\n",
    "    metadata = {\"finding_name\": \"send_email_also_to_subscription_owners_is_set_to_on\",\n",
    "                \"negative_name\": \"send_email_also_to_subscription_owners_is_set_to_on\",\n",
    "                \"columns\": [\"Resource Group\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(security_center_filtered_path)\n",
    "get_data()\n",
    "security_center = load_security_center(security_center_path)\n",
    "security_center[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scanner_dir}/scanner/modules/storage_accounts.py\n",
    "#%load {scanner_dir}/scanner/modules/storage_accounts.py\n",
    "\n",
    "activity_logs_path = os.path.join(raw_data_dir, 'activity_logs.json')\n",
    "storage_accounts_path = os.path.join(raw_data_dir, 'storage_accounts.json')\n",
    "\n",
    "def get_storage_accounts(storage_accounts_path):\n",
    "    \"\"\"\n",
    "    Query Azure api for storage accounts info and save to disk\n",
    "    \"\"\"\n",
    "    storage_accounts = !az storage account list\n",
    "    storage_accounts = yaml.safe_load(storage_accounts.nlstr)\n",
    "        \n",
    "    with open(storage_accounts_path, 'w') as f:\n",
    "        json.dump(storage_accounts, f, indent=4, sort_keys=True)\n",
    "    return storage_accounts\n",
    "\n",
    "def load_storage_accounts(storage_accounts_path):\n",
    "    with open(storage_accounts_path, 'r') as f:\n",
    "        storage_accounts = yaml.safe_load(f)\n",
    "    return storage_accounts\n",
    "\n",
    "activity_logs_starttime_timedelta = datetime.timedelta(days=90)\n",
    "def get_start_time(timedelta=datetime.timedelta(days=90)):\n",
    "    \"\"\"\n",
    "    Given datetime.timedelta(days=days, hours=hours), return string in iso tz format \n",
    "    \"\"\"\n",
    "    return datetime.datetime.strftime(datetime.datetime.now() - timedelta, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def get_activity_logs(activity_logs_path, resource_groups):\n",
    "    activity_logs = {}\n",
    "    start_time = get_start_time(activity_logs_starttime_timedelta)\n",
    "    for resource_group in resource_groups:\n",
    "        resource_group = resource_group['name']\n",
    "        activity_log = !az monitor activity-log list --resource-group {resource_group} --start-time {start_time}\n",
    "        activity_log = yaml.safe_load(activity_log.nlstr)\n",
    "        activity_logs[resource_group] = activity_log\n",
    "    with open(activity_logs_path, 'w') as f:\n",
    "        json.dump(activity_logs, f, indent=4, sort_keys=True)\n",
    "    return activity_logs    \n",
    "\n",
    "def load_activity_logs(activity_logs_path):\n",
    "    with open(activity_logs_path, 'r') as f:\n",
    "        activity_logs = yaml.safe_load(f)\n",
    "    return activity_logs\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "# Tests\n",
    "#################\n",
    "\n",
    "def secure_transfer_required_is_set_to_enabled_3_1(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    for account in storage_accounts:\n",
    "        name = account['name']\n",
    "        resource_group = account['resourceGroup']\n",
    "        enabled = account['enableHttpsTrafficOnly']\n",
    "        if enabled != True:\n",
    "            items_flagged_list.append((resource_group, name))\n",
    "    stats = {'items_flagged': len(items_flagged_list), \"items_checked\": len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"secure_transfer_required_is_set_to_enabled\",\n",
    "                \"negative_name\": \"secure_transfer_required_not_enabled\",\n",
    "                \"columns\": [\"Resource Group\", \"Storage Account Name\"]}\n",
    "    return {\"items\": items_flagged_list, \n",
    "            \"stats\": stats, \n",
    "            \"metadata\": metadata }\n",
    "            \n",
    "\n",
    "def storage_service_encryption_is_set_to_enabled_for_blob_service_3_2(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    for account in storage_accounts:\n",
    "        if account['encryption']['services']['blob'] and (account['encryption']['services']['blob']['enabled'] != True):\n",
    "            items_flagged_list.append((account['resourceGroup'], account['name']))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"storage_service_encryption_is_set_to_enabled_for_blob_service\",\n",
    "                \"negative_name\": \"storage_service_encryption_not_enabled_for_blob_service\",\n",
    "                \"columns\": [\"Resource Group\",\"Storage Account Name\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata }\n",
    "           \n",
    "\n",
    "# may need to run section 6 Networking first to get activity_log\n",
    "def storage_account_access_keys_are_periodically_regenerated_3_3(activity_logs, storage_accounts, resource_groups):\n",
    "    items_flagged_list = []\n",
    "    \n",
    "    max_rotation_days = 90\n",
    "    most_recent_rotations = {}\n",
    "    for resource_group in resource_groups:\n",
    "        resource_group_name = resource_group['name']\n",
    "        for log in activity_logs[resource_group_name]:\n",
    "            if log[\"authorization\"] and (log[\"authorization\"][\"action\"] == \"Microsoft.Storage/storageAccounts/regenerateKey/action\"):\n",
    "                scope = log[\"authorization\"][\"scope\"]\n",
    "                _, _, _, resource_group, _, _, _, storage_account_name = scope.split('/')\n",
    "                timestamp = log[\"eventTimestamp\"]\n",
    "                event_day = timestamp.split('T')[0]\n",
    "                event_day = datetime.datetime.strptime(event_time, '%Y-%m-%d')\n",
    "                status = log[\"status\"][\"localizedValue\"]\n",
    "                if status == \"Success\":\n",
    "                    # fromtimestamp(0) gives smallest date possible in epoch time\n",
    "                    existing_update = most_recent_rotations.get(storage_account, datetime.datetime.fromtimestamp(0))\n",
    "                    most_recent_rotations[storage_account] = max(existing_update, event_time)\n",
    "\n",
    "    for storage_account in storage_accounts:\n",
    "        resource_group = storage_account[\"resourceGroup\"]\n",
    "        storage_account_name = storage_account['name']\n",
    "        items_flagged_list.append((resource_group, storage_account_name, str(most_recent_rotations.get(storage_account_name, \"No rotation\"))))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"storage_account_access_keys_are_periodically_regenerated\",\n",
    "                \"negative_name\": \"storage_account_access_keys_not_periodically_regenerated\",\n",
    "                \"columns\": [\"Resource Group\", \"Storage Account\", \"Rotation Date\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def shared_access_signature_tokens_expire_within_an_hour_3_4(storage_accounts):\n",
    "    \"\"\"\n",
    "    There is no automation possible for this currently\n",
    "    Manual\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def shared_access_signature_tokens_are_allowed_only_over_https_3_5(storage_accounts):\n",
    "    \"\"\"\n",
    "    There is no automation possible for this currently\n",
    "    Manual\n",
    "    \"\"\"\n",
    "    pass\n",
    "                                      \n",
    "def storage_service_encryption_is_set_to_enabled_for_file_service_3_6(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    stats = {}\n",
    "    for account in storage_accounts:\n",
    "        if account['encryption']['services']['file'] and (account['encryption']['services']['file']['enabled'] != True):\n",
    "            items_flagged_list.append((account['name']))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(storage_accounts)}\n",
    "    metadata = {\"finding_name\": \"storage_service_encryption_is_set_to_enabled_for_file_service\",\n",
    "                \"negative_name\": \"storage_service_encryption_not_enabled_for_file_service\",\n",
    "                \"columns\": [\"Storage Account Name\"]}\n",
    "\n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def public_access_level_is_set_to_private_for_blob_containers_3_7(storage_accounts):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    for account in storage_accounts:\n",
    "        account_name = account[\"name\"]\n",
    "        resource_group = account[\"resourceGroup\"]\n",
    "        # get a key that works.  likely this will be a specific key not key[0]\n",
    "        keys = !az storage account keys list --account-name {account_name} --resource-group {resource_group}\n",
    "        keys = yaml.safe_load(keys.nlstr)\n",
    "        key = keys[0]\n",
    "        container_list = !az storage container list --account-name {account_name} --account-key {account_key}\n",
    "        container_list = yaml.load(container_list.nlstr)\n",
    "        for container in container_list:\n",
    "            print(container)\n",
    "            items_checked += 1\n",
    "            public_access = container[\"properties\"][\"public_access\"]\n",
    "            if public_access == True:\n",
    "                items_flagged_list.append((account_name, container))\n",
    "    stats = {'items_flagged': len(items_flagged_list), \"items_checked\": items_checked}\n",
    "    metadata = {\"finding_name\": \"public_access_level_is_set_to_private_for_blob_containers\",\n",
    "                \"negative_name\": \"public_access_level_not_private_for_blob_containers\",\n",
    "                \"columns\": [\"Storage Account Name\", \"Container\"]}\n",
    "    \n",
    "    return {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata }\n",
    "\n",
    "    \n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Generate json for the storage_accounts findings\n",
    "    \"\"\"\n",
    "    resource_groups = get_resource_groups(resource_groups_path)\n",
    "    print(resource_groups)\n",
    "    get_activity_logs(activity_logs_path, resource_groups)\n",
    "    get_storage_accounts(storage_accounts_path)\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Generate filtered (failing) output in json\n",
    "    \"\"\"\n",
    "    resource_groups = load_resource_groups(resource_groups_path)\n",
    "    storage_accounts = load_storage_accounts(storage_accounts_path)\n",
    "    activity_logs = load_activity_logs(activity_logs_path)\n",
    "    \n",
    "    storage_results = {}\n",
    "    storage_results['secure_transfer_required_is_set_to_enabled'] = secure_transfer_required_is_set_to_enabled_3_1(storage_accounts)\n",
    "    storage_results['storage_service_encryption_is_set_to_enabled_for_blob_service'] = storage_service_encryption_is_set_to_enabled_for_blob_service_3_2(storage_accounts)\n",
    "    storage_results['storage_account_access_keys_are_periodically_regenerated'] = storage_account_access_keys_are_periodically_regenerated_3_3(activity_logs, storage_accounts, resource_groups)\n",
    "    storage_results['storage_service_encryption_is_set_to_enabled_for_file_service'] = storage_service_encryption_is_set_to_enabled_for_file_service_3_6(storage_accounts)\n",
    "    #storage_results['public_access_level_is_set_to_private_for_blob_containers'] = public_access_level_is_set_to_private_for_blob_containers_3_7(storage_accounts)\n",
    "        \n",
    "    with open(os.path.join(scan_data_dir, 'filtered', 'storage_accounts_filtered.json'), 'w') as f:\n",
    "        json.dump(storage_results, f, indent=4, sort_keys=True)\n",
    "    return storage_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(storage_accounts_path)\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_finding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If editing utils.py you may need to reload here.\n",
    "# from importlib import reload\n",
    "# reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scanner_dir}/scanner/modules/sql_servers.py\n",
    "#%load {scanner_dir}/scanner/modules/sql_servers.py\n",
    "\n",
    "import utils\n",
    "import yaml\n",
    "\n",
    "sql_servers_path = os.path.join(raw_data_dir, 'sql_servers.json')\n",
    "sql_server_policies_path = os.path.join(raw_data_dir, 'sql_server_policies.json')\n",
    "\n",
    "def get_data():\n",
    "    sql_servers = get_sql_servers(sql_servers_path)\n",
    "    get_sql_server_policies(sql_server_policies_path, sql_servers)\n",
    "\n",
    "def get_sql_servers(sql_servers_path) :\n",
    "    sql_servers_string = utils.call(\"az sql server list\")\n",
    "    sql_servers_json = utils.jsonify(sql_servers_string)\n",
    "    with open(sql_servers_path, 'w') as f:\n",
    "        json.dump(sql_servers_json, f, indent=4, sort_keys=True)\n",
    "    return sql_servers_json\n",
    "\n",
    "def get_sql_server_policies(sql_server_policies_path, sql_servers):\n",
    "    results = {}\n",
    "    subscriptionId = utils.get_subscription_id()\n",
    "    for sql_server in sql_servers:\n",
    "        server_name = sql_server['name']\n",
    "        resource_group = sql_server['resourceGroup']\n",
    "        sql_server_policies = {}\n",
    "        sql_server_policies['audit_policy'] = get_sql_server_audit_policies(subscriptionId, resource_group, server_name)\n",
    "        sql_server_policies['threat_detection_policy'] = get_sql_server_threat_detection_policies(subscriptionId, resource_group, server_name)\n",
    "        sql_server_policies['active_directory_admin_configurations'] = get_sql_server_active_directory_admin_configuration(subscriptionId, resource_group, server_name)\n",
    "        results[(resource_group, server_name)] = sql_server_policies\n",
    "    with open(sql_server_policies_path, 'w') as f:\n",
    "        yaml.dump(results, f)\n",
    "    return results\n",
    "\n",
    "def load_sql_servers(sql_servers_path):\n",
    "    with open(sql_servers_path, 'r') as f:\n",
    "        sql_servers = yaml.load(f)\n",
    "    return sql_servers\n",
    "\n",
    "def load_sql_server_policies(sql_server_policies_path):\n",
    "    with open(sql_server_policies_path, 'r') as f:\n",
    "        sql_server_policies = yaml.load(f)\n",
    "    return sql_server_policies\n",
    "    \n",
    "# This function will be recentered around Azure Command Line, after such an option becomes available.\n",
    "def get_sql_server_audit_policies(subscriptionId, resource_group, server_name):\n",
    "    endpoint = \"https://management.azure.com/subscriptions/\"+subscriptionId+\"/resourceGroups/\"+resource_group+\"/providers/Microsoft.Sql/servers/\"+server_name+\"/auditingSettings/Default?api-version=2015-05-01-preview\"\n",
    "    sql_server_audit_policy = utils.make_request(endpoint)\n",
    "    sql_server_audit_policy = utils.jsonify(sql_server_audit_policy)\n",
    "    return sql_server_audit_policy\n",
    "\n",
    "# This function will be recentered around Azure Command Line, after such an option becomes available.\n",
    "def get_sql_server_threat_detection_policies(subscriptionId, resource_group, server_name):\n",
    "    endpoint = \"https://management.azure.com/subscriptions/\"+subscriptionId+\"/resourceGroups/\"+resource_group+\"/providers/Microsoft.Sql/servers/\"+server_name+\"/securityAlertPolicies/Default?api-version=2015-05-01-preview\"\n",
    "    sql_server_threat_detection_policy = utils.make_request(endpoint)\n",
    "    sql_server_threat_detection_policy = utils.jsonify(sql_server_threat_detection_policy)\n",
    "    return sql_server_threat_detection_policy\n",
    "\n",
    "def get_sql_server_active_directory_admin_configuration(subscriptionId, resource_group, server_name):\n",
    "    active_directory_admin_configuration = utils.call(\"az sql server ad-admin list --resource-group \" + resource_group + \" --server \" + server_name)\n",
    "    active_directory_admin_configuration = utils.jsonify(active_directory_admin_configuration)\n",
    "    return active_directory_admin_configuration\n",
    "\n",
    "##################\n",
    "# Tests\n",
    "##################\n",
    "def wrap(pre, post):\n",
    "    def decorate(func):\n",
    "        def call(*args, **kwargs):\n",
    "            pre(func, *args, **kwargs)\n",
    "            result = func(*args, **kwargs)\n",
    "            post(func, result, results, *args, **kwargs)\n",
    "            return result\n",
    "        return call\n",
    "    return decorate\n",
    "\n",
    "def remove_section_digits(name):\n",
    "    filtered = []\n",
    "    name_words = name.split('_')\n",
    "    # remove trailing digits, taking care not to remove 90 in ...than_90_days_4_1_7\n",
    "    for i, word in enumerate(name_words):\n",
    "        if not word.isdigit() or ( (i < len(name_words)-2) and not name_words[i+1].isdigit()):\n",
    "            filtered.append(word)\n",
    "    return '_'.join(filtered)\n",
    "\n",
    "def trace_in(func, *args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def trace_out(func, result, *args, **kwargs):\n",
    "    name = remove_section_digits(func.__name__)\n",
    "    finding_results = results.get(name, {})\n",
    "    if finding_results:\n",
    "        items_flagged_list = finding_results[\"items\"]\n",
    "        items_checked = finding_results[\"stats\"][\"items_checked\"]\n",
    "    else:\n",
    "        items_flagged_list = []\n",
    "        items_checked = 0\n",
    "    items_checked += 1\n",
    "    if not result:\n",
    "        items_flagged_list.append((kwargs['resource_group'], kwargs['server_name']))\n",
    "           \n",
    "    results[name] = {\"items\": items_flagged_list, \"stats\": {\"items_checked\": items_checked}}\n",
    "        \n",
    "results = {}\n",
    "\n",
    "def test_controls() :\n",
    "    global results\n",
    "    sql_servers = load_sql_servers(sql_servers_path)\n",
    "    sql_server_policies = load_sql_server_policies(sql_server_policies_path)\n",
    "    \n",
    "    for (resource_group, server_name), sql_server_policy in sql_server_policies.items():\n",
    "        sql_server_audit_policy = sql_server_policies[(resource_group, server_name)]['audit_policy']\n",
    "        sql_server_threat_detection_policy = sql_server_policies[(resource_group, server_name)]['threat_detection_policy']\n",
    "        sql_server_active_directory_admin_configurations = sql_server_policies[(resource_group, server_name)]['active_directory_admin_configurations']\n",
    "\n",
    "        auditing_is_set_to_on_4_1_1(sql_server_audit_policy, resource_group=resource_group, server_name=server_name)\n",
    "        threat_detection_is_set_to_on_4_1_2(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        threat_detection_types_is_set_to_all_4_1_3(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        send_alerts_to_is_set_4_1_4(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        email_service_and_co_administrators_is_enabled_4_1_5(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        auditing_retention_is_greater_than_90_days_4_1_6(sql_server_audit_policy, resource_group=resource_group, server_name=server_name)\n",
    "        threat_detection_retention_is_greater_than_90_days_4_1_7(sql_server_threat_detection_policy, resource_group=resource_group, server_name=server_name)\n",
    "        azure_active_directory_admin_is_configured_4_1_8(sql_server_active_directory_admin_configurations, resource_group=resource_group, server_name=server_name)\n",
    "\n",
    "    stats_results = {}\n",
    "    for finding in results:\n",
    "        items_flagged_list = results[finding][\"items\"]\n",
    "        items_checked = results[finding][\"stats\"][\"items_checked\"]\n",
    "        items_flagged = len(items_flagged_list)\n",
    "        stats = {'items_flagged': len(items_flagged_list),\n",
    "                 'items_checked': items_checked}\n",
    "        metadata = {\"finding_name\": finding,\n",
    "                    \"negative_name\": \"\",\n",
    "                    \"columns\": [\"Region\", \"Server\"]}            \n",
    "        stats_results[finding] = {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "        \n",
    "    with open(os.path.join(scan_data_dir, 'filtered', 'sql_servers_filtered.json'), 'w') as f:\n",
    "        yaml.dump(stats_results, f)\n",
    "    # clear results for next run\n",
    "    results = {}\n",
    "    return stats_results\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def auditing_is_set_to_on_4_1_1(sql_server_audit_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_audit_policies[\"properties\"][\"state\"] == \"Disabled\" :\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def threat_detection_is_set_to_on_4_1_2(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" :\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "@wrap(trace_in, trace_out)\n",
    "def threat_detection_types_is_set_to_all_4_1_3(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" or sql_server_threat_detection_policies[\"properties\"][\"disabledAlerts\"] != \"\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def send_alerts_to_is_set_4_1_4(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" or sql_server_threat_detection_policies[\"properties\"][\"emailAddresses\"] != \"\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def email_service_and_co_administrators_is_enabled_4_1_5(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\" or sql_server_threat_detection_policies[\"properties\"][\"emailAccountAdmins\"] != \"\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "@wrap(trace_in, trace_out)\n",
    "def auditing_retention_is_greater_than_90_days_4_1_6(sql_server_audit_policies, resource_group=None, server_name=None):\n",
    "    if (sql_server_audit_policies[\"properties\"][\"state\"] == \"Disabled\"):\n",
    "        return False\n",
    "    retention_days = int(sql_server_audit_policies[\"properties\"][\"retentionDays\"])\n",
    "    if (retention_days) ==0 or (retention_days > 90):\n",
    "        return True\n",
    "    else:    \n",
    "        return False\n",
    "    \n",
    "@wrap(trace_in, trace_out)\n",
    "def threat_detection_retention_is_greater_than_90_days_4_1_7(sql_server_threat_detection_policies, resource_group=None, server_name=None):\n",
    "    if (sql_server_threat_detection_policies[\"properties\"][\"state\"] == \"Disabled\"):\n",
    "        return False\n",
    "    retention_days = int(sql_server_threat_detection_policies[\"properties\"][\"retentionDays\"])\n",
    "    if (retention_days) ==0 or (retention_days > 90):\n",
    "        return True\n",
    "    else:    \n",
    "        return False\n",
    "    \n",
    "@wrap(trace_in, trace_out)\n",
    "def azure_active_directory_admin_is_configured_4_1_8(sql_server_active_directory_admin_configurations, resource_group=None, server_name=None):\n",
    "    if not sql_server_active_directory_admin_configurations:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scanner_dir}/scanner/modules/sql_databases.py\n",
    "#%load {scanner_dir}/scanner/modules/sql_databases.py\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "sql_databases_path = os.path.join(raw_data_dir, 'sql_databases.json')\n",
    "sql_database_policies_path = os.path.join(raw_data_dir, 'sql_database_policies.json')\n",
    "sql_databases_filtered_path = os.path.join(scan_data_dir, 'filtered', 'sql_databases_filtered.json')\n",
    "\n",
    "def get_sql_servers(sql_servers_path):\n",
    "    sql_servers = !az sql server list\n",
    "    sql_servers = yaml.load(sql_servers.nlstr)\n",
    "    with open(sql_servers_path, 'w') as f:\n",
    "        yaml.dump(sql_servers, f)\n",
    "    return sql_servers\n",
    "\n",
    "def load_sql_servers(sql_servers_path):\n",
    "    with open(sql_servers_path, 'r') as f:\n",
    "        sql_servers = yaml.load(f)\n",
    "    return sql_servers\n",
    "\n",
    "def get_dbs(sql_servers, sql_databases_path):\n",
    "    server_dbs = {}\n",
    "    for server in sql_servers:\n",
    "        server_name = server['name']\n",
    "        resource_group = server['resourceGroup']\n",
    "        dbs = !az sql db list --resource-group {resource_group} --server {server_name}\n",
    "        dbs = yaml.load(dbs.nlstr)\n",
    "        server_dbs[(resource_group, server_name)] = dbs\n",
    "        \n",
    "    with open(sql_databases_path, 'w') as f:\n",
    "        yaml.dump(server_dbs, f)\n",
    "    return server_dbs\n",
    "\n",
    "def load_dbs(sql_databases_path):\n",
    "    with open(sql_databases_path, 'r') as f:\n",
    "        server_dbs = yaml.load(f)\n",
    "    return server_dbs\n",
    "\n",
    "def get_sql_database_policies(sql_dbs, sql_database_policies_path):\n",
    "    \"\"\"\n",
    "    For each db in sql_dbs fetch the policies and write to disk\n",
    "    \"\"\"\n",
    "    sql_database_policies = {}\n",
    "    for (resource_group, server_name), dbs in sql_dbs.items():\n",
    "        for db in dbs:\n",
    "            print(db)\n",
    "            db_name = db['name']\n",
    "            if db_name == 'master':\n",
    "                continue\n",
    "            threat_policy = !az sql db threat-policy show --resource-group {resource_group} --server {server_name} --name {db_name}\n",
    "            threat_policy = yaml.load(threat_policy.nlstr)\n",
    "\n",
    "            audit_policy = !az sql db audit-policy show --resource-group {resource_group} --server {server_name} --name {db_name}\n",
    "            audit_policy = yaml.load(audit_policy.nlstr)\n",
    "\n",
    "            tde_policy = !az sql db tde show --resource-group {resource_group} --server {server_name} --database {db_name}\n",
    "            tde_policy = yaml.load(tde_policy.nlstr)\n",
    "\n",
    "            sql_policy = {}\n",
    "            sql_policy['threat'] = threat_policy\n",
    "            sql_policy['audit'] = audit_policy\n",
    "            sql_policy['tde'] = tde_policy\n",
    "            sql_database_policies[(resource_group, server_name, db_name)] = sql_policy\n",
    "        \n",
    "    with open(sql_database_policies_path, 'w') as f:\n",
    "        yaml.dump(sql_database_policies, f)\n",
    "    return sql_database_policies\n",
    "\n",
    "def load_sql_database_policies(sql_policies_path):\n",
    "    \"\"\"\n",
    "    Load sql database policies\n",
    "    \"\"\"\n",
    "    with open(sql_database_policies_path, 'r') as f:\n",
    "        sql_database_policies = yaml.load(f)\n",
    "    return sql_database_policies\n",
    "\n",
    "################\n",
    "# Tests\n",
    "################\n",
    "\n",
    "def remove_section_digits(name):\n",
    "    filtered = []\n",
    "    name_words = name.split('_')\n",
    "    # remove trailing digits, taking care not to remove 90 in ...than_90_days_4_1_7\n",
    "    for i, word in enumerate(name_words):\n",
    "        if not word.isdigit() or ( (i < len(name_words)-2) and not name_words[i+1].isdigit()):\n",
    "            filtered.append(word)\n",
    "    return '_'.join(filtered)\n",
    "    \n",
    "results = {}\n",
    "def test_controls():\n",
    "    global results\n",
    "    def wrap(pre, post):\n",
    "        global results\n",
    "        def decorate(func):\n",
    "            global results\n",
    "            def call(*args, **kwargs):\n",
    "                global results\n",
    "                pre(func, *args, **kwargs)\n",
    "                result = func(*args, **kwargs)\n",
    "                post(func, result, results, *args, **kwargs)\n",
    "                return result\n",
    "            return call\n",
    "        return decorate\n",
    "    \n",
    "    def remove_section_digits(name):\n",
    "        return '_'.join([item for item in name.split('_') if not item.isdigit()])\n",
    "\n",
    "    def trace_in(func, *args, **kwargs):\n",
    "        global results\n",
    "        pass\n",
    "    \n",
    "    def trace_out(func, result, *args, **kwargs):\n",
    "        global results\n",
    "        name = remove_section_digits(func.__name__)\n",
    "        finding_results = results.get(name, {})\n",
    "        if finding_results:\n",
    "            items_flagged_list = finding_results[\"items\"]\n",
    "            items_checked = finding_results[\"stats\"][\"items_checked\"]\n",
    "        else:\n",
    "            items_flagged_list = []\n",
    "            items_checked = 0\n",
    "        items_checked += 1\n",
    "        if not result:\n",
    "            items_flagged_list.append((resource_group, server_name, db))\n",
    "           \n",
    "        results[name] = {\"items\": items_flagged_list, \"stats\": {\"items_checked\": items_checked}}\n",
    "\n",
    "\n",
    "    @wrap(trace_in, trace_out)\n",
    "    def auditing_is_set_to_on_4_2_1(audit_policy):\n",
    "        if audit_policy['state'] != 'Enabled':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "  \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def threat_detection_is_set_to_on_4_2_2(threat_policy):\n",
    "        if threat_policy['state'] != 'Enabled':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def threat_detection_types_is_set_to_all_4_2_3(threat_policy):\n",
    "        if threat_policy['disabledAlerts'] in ['All', '']:\n",
    "            return True\n",
    "        else:\n",
    "            print('threat_detection_types_is_set_to_all_4_2_3 disabledAlerts', threat_policy['disabledAlerts'])\n",
    "            return False\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def send_alerts_to_is_set_4_2_4(threat_policy):\n",
    "        if threat_policy['emailAddresses']:\n",
    "            return set(threat_policy['emailAddresses'])\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def email_service_and_co_administrators_is_enabled_4_2_5(threat_policy):\n",
    "        if threat_policy['emailAccountAdmins'] != \"Enabled\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    @wrap(trace_in, trace_out)\n",
    "    def data_encryption_is_set_to_on_4_2_6(tde_policy):\n",
    "        if tde_policy['status'] != \"Enabled\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    @wrap(trace_in, trace_out)\n",
    "    def auditing_retention_is_greater_than_90_days_4_2_7(audit_policy):\n",
    "        if (audit_policy['retentionDays'] > 0) and (audit_policy['retentionDays'] <= 90):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    @wrap(trace_in, trace_out)\n",
    "    def threat_retention_is_greater_than_90_days_4_2_8(threat_policy):\n",
    "        if (threat_policy['retentionDays'] > 0) and (threat_policy['retentionDays'] == 0) <= 90:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    sql_database_policies = load_sql_database_policies(sql_database_policies_path)\n",
    "    for (resource_group, server_name, db), sql_database_policy in sql_database_policies.items():\n",
    "        \n",
    "        audit_policy = sql_database_policy['audit']\n",
    "        threat_policy = sql_database_policy['threat']\n",
    "        tde_policy = sql_database_policy['tde']            \n",
    "        \n",
    "        auditing_is_set_to_on_4_2_1(audit_policy)\n",
    "        threat_detection_is_set_to_on_4_2_2(threat_policy)\n",
    "        threat_detection_types_is_set_to_all_4_2_3(threat_policy)\n",
    "        send_alerts_to_is_set_4_2_4(threat_policy)\n",
    "        email_service_and_co_administrators_is_enabled_4_2_5(threat_policy)\n",
    "        data_encryption_is_set_to_on_4_2_6(tde_policy)\n",
    "        auditing_retention_is_greater_than_90_days_4_2_7(audit_policy)\n",
    "        threat_retention_is_greater_than_90_days_4_2_8(threat_policy)\n",
    "\n",
    "    stats_results = {}\n",
    "    for finding in results:\n",
    "        items_flagged_list = results[finding][\"items\"]\n",
    "        items_checked = results[finding][\"stats\"][\"items_checked\"]\n",
    "        items_flagged = len(items_flagged_list)\n",
    "        stats = {'items_flagged': len(items_flagged_list),\n",
    "                 'items_checked': items_checked}\n",
    "        metadata = {\"finding_name\": finding,\n",
    "                    \"negative_name\": \"\",\n",
    "                    \"columns\": [\"Region\", \"Server\", \"Database\"]}            \n",
    "        stats_results[finding] = {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "    \n",
    "    with open(sql_databases_filtered_path, 'w') as f:\n",
    "        yaml.dump(stats_results, f)\n",
    "    # clear results for next run\n",
    "    results = {}\n",
    "    return stats_results\n",
    "\n",
    "def get_data():\n",
    "    sql_servers = get_sql_servers(sql_servers_path)\n",
    "    sql_dbs = get_dbs(sql_servers, sql_databases_path)\n",
    "    \n",
    "    sql_database_policies = get_sql_database_policies(sql_dbs, sql_database_policies_path)\n",
    "\n",
    "    return {\"sql_servers\": sql_servers, \"sql_databases\": sql_dbs, \"sql_database_policies\": sql_database_policies}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sql_databases_path)\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Logging and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az monitor diagnostic-settings list --resource  \"/subscriptions/6ff7f744-b7aa-4894-94b3-0ed92b8b2866/resourceGroups/RG-SC-DEV-SS/providers/Microsoft.KeyVault/vaults/AZDEVKV01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scanner_dir}/scanner/modules/logging_and_monitoring.py\n",
    "#%load {scanner_dir}/scanner/modules/logging_and_monitoring.py\n",
    "\n",
    "# Generate files in raw_data_dir\n",
    "\n",
    "monitor_diagnostic_settings_path = os.path.join(raw_data_dir, 'monitor_diagnostic_settings.json')\n",
    "activity_logs_path = os.path.join(raw_data_dir, 'activity_logs.json')\n",
    "resource_groups_path = os.path.join(raw_data_dir, \"resource_groups.json\")\n",
    "\n",
    "resource_ids_for_diagnostic_settings_path = os.path.join(raw_data_dir, 'resource_ids_for_diagnostic_settings.json')\n",
    "resource_diagnostic_settings_path = os.path.join(raw_data_dir, 'resource_diagnostic_settings.json')\n",
    "\n",
    "logging_and_monitoring_filtered_path = os.path.join(filtered_data_dir, 'logging_and_monitoring_filtered.json')\n",
    "\n",
    "def get_resource_ids_for_diagnostic_settings():\n",
    "    resource_ids = []\n",
    "    # Other resource_ids could be gathered.  So far, only keyvault\n",
    "    keyvaults = !az keyvault list    \n",
    "    keyvaults = yaml.load(keyvaults.nlstr)\n",
    "    for keyvault in keyvaults:\n",
    "        resource_ids.append(keyvault['id'])\n",
    "    with open(resource_ids_for_diagnostic_settings_path, 'w') as f:\n",
    "        json.dump(resource_ids, f, indent=4, sort_keys=True)\n",
    "    return resource_ids\n",
    "\n",
    "def load_resource_ids_for_diagnostic_settings(resource_ids_for_diagnostic_settings_path):\n",
    "    with open(resource_ids_for_diagnostic_settings_path, 'r') as f:\n",
    "        resource_ids_for_diagnostic_settings = yaml.load(f)\n",
    "    return resource_ids_for_diagnostic_settings\n",
    "\n",
    "def get_resource_diagnostic_settings(resource_ids_for_diagnostic_settings):\n",
    "    keyvault_settings_list = []\n",
    "    for resource_id in resource_ids_for_diagnostic_settings:\n",
    "        keyvault_settings = !az monitor diagnostic-settings list --resource {resource_id}\n",
    "        keyvault_settings = yaml.load(keyvault_settings.nlstr)\n",
    "        *prefix, resource_group, _, _, _, keyvault_name = resource_id.split('/')\n",
    "        if not keyvault_settings['value']:\n",
    "            keyvault_settings['value'].append({'keyvault_name': keyvault_name, 'resourceGroup': resource_group})\n",
    "        else:\n",
    "            for setting in keyvault_settings['value']:\n",
    "                setting['keyvault_name'] = keyvault_name\n",
    "        print(keyvault_settings)\n",
    "        keyvault_settings_list.append(keyvault_settings)\n",
    "        \n",
    "    with open(resource_diagnostic_settings_path, 'w') as f:\n",
    "        yaml.dump(keyvault_settings_list, f)\n",
    "    return resource_ids_for_diagnostic_settings \n",
    "\n",
    "def load_resource_diagnostic_settings(resource_diagnostic_settings_path):\n",
    "    with open(resource_diagnostic_settings_path, 'r') as f:\n",
    "        resource_diagnostic_settings = yaml.load(f)\n",
    "    return resource_diagnostic_settings        \n",
    "        \n",
    "def get_monitor_diagnostic_settings(monitor_diagnostic_settings_path, resource_ids):\n",
    "    \"\"\"\n",
    "    @monitor_diagnostic_settings_path: string - path to output json file\n",
    "    @returns: list of activity_log_alerts dicts\n",
    "    \"\"\"\n",
    "    monitor_diagnostic_settings_results = {}\n",
    "    for resource_id in resource_ids:\n",
    "        monitor_diagnostic_settings = !az monitor diagnostic-settings list --resource {resource_id}\n",
    "        monitor_diagnostic_settings = yaml.load(monitor_diagnostic_settings.nlstr)\n",
    "        monitor_diagnostic_settings_results[resource_id] = monitor_diagnostic_settings\n",
    "    with open(monitor_diagnostic_settings_path, 'w') as f:\n",
    "        json.dump(monitor_diagnostic_settings_results, f, indent=4, sort_keys=True)\n",
    "    return monitor_diagnostic_settings_results\n",
    "\n",
    "def load_monitor_diagnostic_settings(monitor_diagnostic_settings_path):\n",
    "    with open(monitor_diagnostic_settings_path, 'r') as f:\n",
    "        monitor_diagnostic_settings = yaml.load(f)\n",
    "    return monitor_diagnostic_settings\n",
    "\n",
    "monitor_log_profiles_path = os.path.join(raw_data_dir, 'monitor_log_profiles.json')\n",
    "\n",
    "def get_monitor_log_profiles(monitor_log_profiles_path):\n",
    "    monitor_log_profiles = !az monitor log-profiles list\n",
    "    monitor_log_profiles = yaml.load(monitor_log_profiles.nlstr)\n",
    "    with open(monitor_log_profiles_path, 'w') as f:\n",
    "        json.dump(monitor_log_profiles, f, indent=4, sort_keys=True)\n",
    "    return monitor_log_profiles\n",
    "\n",
    "def load_monitor_log_profiles(monitor_log_profiles_path):\n",
    "    with open(monitor_log_profiles_path, 'r') as f:\n",
    "        monitor_log_profiles = yaml.load(f)\n",
    "    return monitor_log_profiles\n",
    "\n",
    "\n",
    "activity_logs_starttime_timedelta = datetime.timedelta(days=90)\n",
    "def get_start_time(timedelta=datetime.timedelta(days=90)):\n",
    "    \"\"\"\n",
    "    Given datetime.timedelta(days=days, hours=hours), return string in iso tz format \n",
    "    \"\"\"\n",
    "    return datetime.datetime.strftime(datetime.datetime.now() - timedelta, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def get_activity_logs(activity_logs_path, resource_groups):\n",
    "    activity_logs = {}\n",
    "    start_time = get_start_time(activity_logs_starttime_timedelta)\n",
    "    for resource_group in resource_groups:\n",
    "        resource_group = resource_group['name']\n",
    "        activity_log = !az monitor activity-log list --resource-group {resource_group} --start-time {start_time}\n",
    "        activity_log = yaml.load(activity_log.nlstr)\n",
    "        activity_logs[resource_group] = activity_log\n",
    "    with open(activity_logs_path, 'w') as f:\n",
    "        json.dump(activity_logs, f, indent=4, sort_keys=True)\n",
    "    return activity_logs    \n",
    "\n",
    "def load_activity_logs(activity_logs_path):\n",
    "    with open(activity_logs_path, 'r') as f:\n",
    "        activity_log = yaml.load(f)\n",
    "    return activity_log\n",
    "\n",
    "activity_log_alerts_path = os.path.join(raw_data_dir, 'activity_log_alerts.json')\n",
    "\n",
    "def get_activity_log_alerts(activity_log_alerts_path):\n",
    "    activity_log_alerts = !az monitor activity-log alert list\n",
    "    activity_log_alerts = yaml.load(activity_log_alerts.nlstr)\n",
    "    with open(activity_log_alerts_path, 'w') as f:\n",
    "        json.dump(activity_log_alerts, f, indent=4, sort_keys=True)\n",
    "    return activity_log_alerts   \n",
    "\n",
    "def load_activity_log_alerts(activity_log_alerts_path):\n",
    "    with open(activity_log_alerts_path, 'r') as f:\n",
    "        activity_log_alerts = yaml.load(f)\n",
    "    return activity_log_alerts\n",
    "\n",
    "def get_data():\n",
    "    resource_ids_for_diagnostic_settings = get_resource_ids_for_diagnostic_settings()\n",
    "    resource_groups = get_resource_groups(resource_groups_path)\n",
    "    get_monitor_log_profiles(monitor_log_profiles_path)\n",
    "    get_monitor_diagnostic_settings(monitor_diagnostic_settings_path, resource_ids_for_diagnostic_settings)\n",
    "    get_activity_log_alerts(activity_log_alerts_path)\n",
    "    get_activity_logs(activity_logs_path, resource_groups)\n",
    "    get_resource_diagnostic_settings(resource_ids_for_diagnostic_settings)\n",
    "\n",
    "    \n",
    "    \n",
    "##################\n",
    "# Tests\n",
    "##################\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Use the data in raw_data_dir or in memory to run tests.\n",
    "    Filtered output of raw_data_dir for failing systems is placed in filtered_data_dir\n",
    "    \"\"\"\n",
    "    resource_ids_for_diagnostic_settings = load_resource_ids_for_diagnostic_settings(resource_ids_for_diagnostic_settings_path)\n",
    "    resource_diagnostic_settings = load_resource_diagnostic_settings(resource_diagnostic_settings_path)\n",
    "    resource_groups = load_resource_groups(resource_groups_path)\n",
    "    monitor_log_profiles = load_monitor_log_profiles(monitor_log_profiles_path)\n",
    "    monitor_diagnostic_settings = load_monitor_diagnostic_settings(monitor_diagnostic_settings_path)\n",
    "    activity_log_alerts = load_activity_log_alerts(activity_log_alerts_path)\n",
    "    activity_logs = load_activity_logs(activity_logs_path)\n",
    "    \n",
    "    \n",
    "    results = {}\n",
    "    results['a_log_profile_exists'] = a_log_profile_exists_5_1(monitor_log_profiles)\n",
    "    results['activity_log_retention_is_set_365_days_or_greater'] = activity_log_retention_is_set_365_days_or_greater_5_2(monitor_log_profiles)\n",
    "    results['activity_log_alert_is_configured'] = activity_log_alert_is_configured(activity_log_alerts, log_alert_policies)\n",
    "    results['logging_for_azure_keyvault_is_enabled'] = logging_for_azure_keyvault_is_enabled_5_13(resource_diagnostic_settings)\n",
    "\n",
    "    with open(logging_and_monitoring_filtered_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, sort_keys=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def a_log_profile_exists_5_1(monitor_log_profiles):\n",
    "    items_flagged_list = []\n",
    "    if monitor_log_profiles:\n",
    "        pass\n",
    "    else:\n",
    "        items_flagged_list.append((\"No log profile\"))\n",
    "        \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': min(1, len(items_flagged_list))}\n",
    "    metadata = {\"finding_name\": \"a_log_profile_exists\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Monitor Log Profile\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "    \n",
    "\n",
    "# Todo, untested as we have [] for log-profiles\n",
    "#@gen_results(results)\n",
    "def activity_log_retention_is_set_365_days_or_greater_5_2(monitor_log_profiles):\n",
    "    items_flagged_list = []\n",
    "    if monitor_log_profiles:\n",
    "        for monitor_log_profile in monitor_log_profiles:\n",
    "            days = monitor_log_profile.get('retentionPolicy', {}).get('days', -1)\n",
    "            if monitor_log_profile.get('retentionPolicy', {}).get('days') <= MIN_ACTIVITY_LOG_RETENDION_DAYS:\n",
    "                items_flagged_list.append((monitor_log_profile['id'], monitor_log_profile['storageAccountId'], days))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(monitor_log_profiles) if monitor_log_profiles else 1}\n",
    "    metadata = {\"finding_name\": \"activity_log_retention_is_set_365_days_or_greater\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Monitor Log Profile\", \"Retention Days\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "# 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 5.10, 5.11, 5.12    \n",
    "log_alert_policies_str = '''\n",
    "- alert_name: 'create_policy_assignment'\n",
    "  operation_name: 'Microsoft.Authorization/policyAssignments/write'\n",
    "  present: False\n",
    "- alert_name: 'create_or_update_network_security_group'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_network_security_group'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/delete'\n",
    "  present: False\n",
    "- alert_name: 'create_or_update_network_security_group_rule'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/securityRules/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_network_security_group_rule'\n",
    "  operation_name: 'Microsoft.Network/networkSecurityGroups/securityRules/delete'\n",
    "  present: False\n",
    "- alert_name: 'create_or_update_security_solution'\n",
    "  operation_name: 'Microsoft.Security/securitySolutions/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_security_solution'\n",
    "  operation_name: 'Microsoft.Security/securitySolutions/delete'\n",
    "  present: False\n",
    "- alert_name: 'update_or_create_SQL_server_firewall_rule'\n",
    "  operation_name: 'Microsoft.Sql/servers/firewallRules/write'\n",
    "  present: False\n",
    "- alert_name: 'delete_SQL_server_firewall_rule'\n",
    "  operation_name: 'Microsoft.Sql/servers/firewallRules/delete'\n",
    "  present: False\n",
    "- alert_name: 'update_security_policy'\n",
    "  operation_name: 'Microsoft.Security/policies/write'\n",
    "  present: False\n",
    "'''\n",
    "log_alert_policies = yaml.load(log_alert_policies_str)\n",
    "\n",
    "def activity_log_alert_is_configured(activity_log_alerts, log_alert_policies):\n",
    "    \"\"\"\n",
    "    #TODO WIP\n",
    "    For each resource_group determine if activity-log alerts are configured correctly\n",
    "    @returns: list of [resource_group, True of False for 5.3 to 5.12 in succession]\n",
    "    \"\"\"\n",
    "    items_flagged_list = []\n",
    "\n",
    "  \n",
    "    for log_alert in activity_log_alerts:\n",
    "        condition = log_alert.get('condition', [])\n",
    "        if not condition:\n",
    "            continue\n",
    "        conditions = condition.get('allOf', [])\n",
    "        if not conditions:\n",
    "            continue\n",
    "        for condition in conditions:\n",
    "            for log_alert_policy in log_alert_policies:\n",
    "                if condition.get('equals') and (condition.get('equals') == log_alert_policy[\"operation_name\"]):\n",
    "                    log_alert_policy[\"present\"] = True\n",
    "\n",
    "    for log_alert_policy in log_alert_policies:\n",
    "        if log_alert_policy['present'] == False:\n",
    "            items_flagged_list.append((log_alert_policy['alert_name'], log_alert_policy['operation_name']))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(log_alert_policies)}\n",
    "    metadata = {\"finding_name\": \"activity_log_alert_is_configured\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Missing Policy\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "                                                                    \n",
    "#@gen_results(results)\n",
    "MIN_ACTIVITY_LOG_RETENDION_DAYS = 365\n",
    "MIN_KEY_VAULT_RETENTION_DAYS = 180\n",
    "def logging_for_azure_keyvault_is_enabled_5_13(resource_diagnostic_settings):        \n",
    "    items_flagged_list = []\n",
    "    for setting in resource_diagnostic_settings:\n",
    "        keyvault_settings_values = setting['value']\n",
    "        if keyvault_settings_values:\n",
    "            for value in keyvault_settings_values:\n",
    "                # Do we need to loop over ['logs'] list as well?  My lists are length 1, only checking [0]\n",
    "                keyvault_name = value['keyvault_name']\n",
    "                resource_group = value['resourceGroup']\n",
    "                if not value.get('logs', None):\n",
    "                    enabled = False\n",
    "                    retention_enabled = False\n",
    "                    retention_days = 0\n",
    "                else:\n",
    "                    enabled = value['logs'][0]['enabled']\n",
    "                    retention_enabled = value['logs'][0]['retentionPolicy']['enabled']\n",
    "                    retention_days = value['logs'][0]['retentionPolicy']['days']                    \n",
    "                if not (enabled and retention_enabled and (retention_days >= MIN_KEY_VAULT_RETENTION_DAYS)):\n",
    "                    items_flagged_list.append((keyvault_name, enabled, retention_enabled, retention_days))\n",
    "        else:\n",
    "            items_flagged_list.append((resource_group, keyvault_name, \"False\", \"False\", \"None\"))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(resource_diagnostic_settings)}\n",
    "    metadata = {\"finding_name\": \"logging_for_azure_keyvault_is_enabled\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Keyvault\", \"Enabled\", \"Retention Enabled\", \"Retention Days\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Networking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scanner_dir}/scanner/modules/logging_and_monitoring.py\n",
    "#%load {scanner_dir}/scanner/modules/logging_and_monitoring.py\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "##########################\n",
    "# Get Raw Data\n",
    "##########################\n",
    "\n",
    "network_flows_path = os.path.join(raw_data_dir, \"network_flows.json\")\n",
    "networking_filtered_path = os.path.join(scan_data_dir, 'filtered', 'networking_filtered.json')\n",
    "\n",
    "network_security_groups_path = os.path.join(raw_data_dir, \"network_security_groups.json\")\n",
    "\n",
    "def get_data():\n",
    "    get_resource_groups(resource_groups_path)\n",
    "    network_security_groups = get_network_security_groups(network_security_groups_path)\n",
    "    get_network_watcher(network_watcher_path)\n",
    "    get_network_flows(network_flows_path, network_security_groups)\n",
    "    \n",
    "def get_network_security_groups(network_security_groups_path):\n",
    "    \"\"\"\n",
    "    @network_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    network_security_groups = !az network nsg list\n",
    "    network_security_groups = yaml.load(network_security_groups.nlstr)\n",
    "    with open(network_security_groups_path, 'w') as f:\n",
    "        json.dump(network_security_groups, f, indent=4, sort_keys=True)\n",
    "    return network_security_groups\n",
    "\n",
    "def load_network_security_groups(network_security_groups_path):\n",
    "    with open(network_security_groups_path, 'r') as f:\n",
    "        network_security_groups = yaml.load(f)\n",
    "    return network_security_groups\n",
    "\n",
    "network_watcher_path = os.path.join(raw_data_dir, \"network_watcher.json\")\n",
    "approved_regions = []\n",
    "def get_network_watcher(network_watcher_path):\n",
    "    \"\"\"\n",
    "    @network_watcher_path: string - path to output json file\n",
    "    \"\"\"\n",
    "    network_watcher = !az network watcher list\n",
    "    network_watcher = yaml.load(network_watcher.nlstr)\n",
    "    with open(network_watcher_path, 'w') as f:\n",
    "        json.dump(network_watcher, f, indent=4, sort_keys=True)\n",
    "    return network_watcher\n",
    "\n",
    "def load_network_watcher(network_watcher_path):\n",
    "    with open(network_watcher_path, 'r') as f:\n",
    "        network_watcher = yaml.load(f)\n",
    "    return network_watcher\n",
    "\n",
    "def get_network_flows(network_flows_path, network_security_groups):\n",
    "    \"\"\"\n",
    "    @network_flows_path: string - path to output json file\n",
    "    @network_security_groups: list of nsgs\n",
    "    @returns: list of network flow dicts\n",
    "    \"\"\"\n",
    "    network_flows = []\n",
    "    for nsg in network_security_groups:\n",
    "        resource_group = nsg['resourceGroup']\n",
    "        nsg_id = nsg['id']\n",
    "        network_flow = !az network watcher flow-log show --resource-group {resource_group} --nsg {nsg_id}\n",
    "        network_flow = yaml.load(network_flow.nlstr)\n",
    "        nsg_name = nsg[\"name\"]\n",
    "        network_flows.append({\"resource_group\": resource_group, \"nsg_name\": nsg_name, \"network_flow\": network_flow})\n",
    "        \n",
    "    with open(network_flows_path, 'w') as f:\n",
    "        json.dump(network_flows, f, indent=4, sort_keys=True)\n",
    "    return network_flows\n",
    "\n",
    "def load_network_flows(network_flows_path):\n",
    "    with open(network_flows_path, 'r') as f:\n",
    "        network_flows = yaml.load(f)\n",
    "    return network_flows\n",
    "\n",
    "##########################\n",
    "# Tests\n",
    "##########################\n",
    "\n",
    "def test_controls():\n",
    "    \"\"\"\n",
    "    Generate filtered (failing) output in json\n",
    "    \"\"\"\n",
    "    network_watcher = load_network_watcher(network_watcher_path)\n",
    "    network_security_groups = load_network_security_groups(network_security_groups_path)\n",
    "    resource_groups = load_resource_groups(resource_groups_path)\n",
    "    network_flows = load_network_flows(network_flows_path)\n",
    "    networking_results = {}\n",
    "\n",
    "    networking_results['access_is_restricted_from_the_internet'] = access_is_restricted_from_the_internet_6_1(network_security_groups)\n",
    "    networking_results['network_security_group_flow_log_retention_period_is_greater_than_90_days'] = network_security_group_flow_log_retention_period_is_greater_than_90_days_6_4(network_flows)\n",
    "    networking_results['network_watcher_is_enabled'] = network_watcher_is_enabled_6_5(network_watcher)\n",
    "                \n",
    "    with open(networking_filtered_path, 'w') as f:\n",
    "        json.dump(networking_results, f, indent=4, sort_keys=True)\n",
    "    return networking_results\n",
    "\n",
    "# 6.1, 6.2, \n",
    "def access_is_restricted_from_the_internet_6_1(network_security_groups):\n",
    "    items_flagged_list = []\n",
    "    for nsg in network_security_groups:\n",
    "        # should actually be any port range that includes 3389\n",
    "        security_rules = nsg['securityRules']\n",
    "        for security_rule in security_rules:\n",
    "            if security_rule['destinationPortRange'] == '3389' and security_rule['direction'] == 'Inbound' and security_rule['protocol'] == 'TCP':\n",
    "                if security_rule['sourceAddressPrefix'] in ['*', '/0', 'internet', 'any']:\n",
    "                    items_flagged_list.append((nsg['resourceGroup'],nsg['name'], '3389', security_rule))\n",
    "            if security_rule['destinationPortRange'] == '22' and security_rule['direction'] == 'Inbound':\n",
    "                if security_rule['sourceAddressPrefix'] in ['*', '/0', 'internet', 'any']:\n",
    "                    items_flagged_list.append((nsg['resourceGroup'],nsg['name'], '22', security_rule))\n",
    "                    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(network_security_groups)}\n",
    "    metadata = {\"finding_name\": \"access_is_restricted_from_the_internet\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"NSG\", \"Port\", \"Rule\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "\n",
    "def sql_server_access_is_restricted_from_the_internet_6_3():\n",
    "    \"\"\"\n",
    "    Powershell\n",
    "    \"\"\"\n",
    "    pass                \n",
    "\n",
    "def network_security_group_flow_log_retention_period_is_greater_than_90_days_6_4(network_flows):\n",
    "    items_flagged_list = []\n",
    "    for network_flow in network_flows:\n",
    "        flow = network_flow['network_flow']\n",
    "        if flow['enabled'] == False:\n",
    "            status = \"Not enabled\"\n",
    "            items_flagged_list.append((network_flow['resource_group'], network_flow['nsg_name'], status))\n",
    "        elif flow['retentionPolicy']['days'] == 0:\n",
    "            continue\n",
    "        elif (flow['retentionPolicy']['days'] < 90) or (flow['retentionPolicy']['enabled'] == False):\n",
    "            status(\"Days {}, Enabled {}\".format(flow['retentionPolicy']['days'], flow['retentionPolicy']['enabled']))\n",
    "            items_flagged_list.append((network_flow['resource_group'], network_flow['nsg_name'], status))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(network_flows)}\n",
    "    metadata = {\"finding_name\": \"network_security_group_flow_log_retention_period_is_greater_than_90_days\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Network Flow\", \"Status\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def network_watcher_is_enabled_6_5(network_watcher):\n",
    "    items_flagged_list = []    \n",
    "    for watcher in network_watcher:\n",
    "        if watcher['provisioningState'] != 'Succeeded':\n",
    "            items_flagged_list.append((watcher))\n",
    "            \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(network_watcher)}\n",
    "    metadata = {\"finding_name\": \"network_security_group_flow_log_retention_period_is_greater_than_90_days\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Network Flow\", \"Status\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware of permission errors visible from the cli\n",
    "\n",
    "$ az network watcher flow-log show -g <my-group> --nsg <my-nsg>\n",
    "The client 'kesten.Broughton@texascapitalbank.com' with object id 'abdcxxx-asdlj-fdskl-yyy-asfd17' does not have authorization to perform action 'Microsoft.Network/networkWatchers/queryFlowLogStatus/action' over scope '/subscriptions/exxxxxx-cbbbb-4444-bbbb-aaaaaaa2e3/resourceGroups/NetworkWatcherRG/providers/Microsoft.Network/networkWatchers/NetworkWatcher_southcentralus'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_finding(findings_template_path, r_parsed, 'Virtual Machines', 2, output='', findings_output_path=findings_out_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /praetorian-tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual Machines\n",
    "\n",
    "filtered_virtual_machines_path = os.path.join(filtered_data_dir, 'virtual_machines_filtered.json')\n",
    "virtual_machines_path = os.path.join(raw_data_dir, 'virtual_machines.json')\n",
    "\n",
    "def get_virtual_machines(virtual_machines_path):\n",
    "    \"\"\"\n",
    "    @virtual_machines_path: string - path to output json file\n",
    "    @returns: list of virtual_machines dict\n",
    "    \"\"\"\n",
    "    virtual_machines = !az vm list\n",
    "    virtual_machines = yaml.load(virtual_machines.nlstr)\n",
    "    with open(virtual_machines_path, 'w') as f:\n",
    "        json.dump(virtual_machines, f, indent=4, sort_keys=True)\n",
    "    return virtual_machines\n",
    "\n",
    "def load_virtual_machines(virtual_machines_path):\n",
    "    with open(virtual_machines_path, 'r') as f:\n",
    "        virtual_machines = yaml.load(f)\n",
    "    return virtual_machines\n",
    "\n",
    "def get_data():\n",
    "    get_virtual_machines(virtual_machines_path)\n",
    "\n",
    "def vm_agent_is_installed_7_1(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    for vm in virtual_machines:\n",
    "        has_agent = False\n",
    "        if vm['resources']:\n",
    "            for resource in vm[\"resources\"]:\n",
    "                if ((vm['resources'][0]['virtualMachineExtensionType'] == 'MicrosoftMonitoringAgent') and (vm['resources'][0]['provisioningState'] == 'Succeeded')):\n",
    "                    has_agent = True\n",
    "        if has_agent:\n",
    "            items_flagged_list.append((vm['resourceGroup'], vm['name']))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"vm_agent_is_installed\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def os_disk_is_encrypted_7_2(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    for vm in virtual_machines:\n",
    "        if vm['storageProfile']['osDisk']['encryptionSettings']:\n",
    "            if not (vm['storageProfile']['osDisk']['encryptionSettings']['enabled'] == True):\n",
    "                items_flagged_list.append((vm['resourceGroup'], vm['name'], vm['storageProfile']['osDisk']['name']))\n",
    "                items_checked += 1\n",
    "        else:\n",
    "            items_flagged_list.append((vm['resourceGroup'], vm['name'], vm['storageProfile']['osDisk']['name']))\n",
    "            items_checked += 1\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"os_disk_is_encrypted\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\", \"Disk Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def data_disks_are_encrypted_7_3(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    for vm in virtual_machines:\n",
    "        name = vm['name']\n",
    "        resource_group = vm['resourceGroup']\n",
    "#         encrypted = !az vm encryption show --name {name} --resource-group {resource_group} --query dataDisk\n",
    "#         encrypted = yaml.load(encrypted.nlstr)\n",
    "#         if encrypted != \"Encrypted\":\n",
    "#             items_flagged_list.append((vm['resourceGroup'], vm['name']))\n",
    "        for disk in vm['storageProfile']['dataDisks']:\n",
    "            if disk['encryptionSettings'] == None:\n",
    "                items_flagged_list.append((vm['resourceGroup'], vm['name'], disk['name']))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"data_disks_are_encrypted\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\", \"Disk Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "\n",
    "def only_approved_extensions_are_installed_7_4(virtual_machines):\n",
    "    # items in the following list do not imply failure, but require review\n",
    "    items_flagged_list = []\n",
    "    approved_extensions = [\n",
    "        'AzureDiskEncryption',\n",
    "        'IaaSAntimalware',\n",
    "        'IaaSDiagnostics',\n",
    "        'MicrosoftMonitoringAgent',\n",
    "        'SqlIaaSAgent',\n",
    "        'OmsAgentForLinux', \n",
    "        'VMAccessForLinux',\n",
    "    ]\n",
    "    for vm in virtual_machines:\n",
    "        name = vm['name']\n",
    "        resource_group = vm['resourceGroup']\n",
    "        extensions = !az vm extension list --vm-name {name} --resource-group {resource_group}\n",
    "        extensions = yaml.load(extensions.nlstr)\n",
    "        for extension in extensions:\n",
    "            if extension['virtualMachineExtensionType'] not in approved_extensions:\n",
    "                items_flagged_list.append((resource_group, name, extension['virtualMachineExtensionType']))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"only_approved_extensions_are_installed\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"VM Name\", \"Extension Name\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def latest_patches_for_all_virtual_machines_are_applied_7_5(virtual_machines):\n",
    "    pass\n",
    "\n",
    "def endpoint_protection_for_all_virtual_machines_is_installed_7_6(virtual_machines):\n",
    "    items_flagged_list = []\n",
    "    accepted_protections = set(['EndpointSecurity', 'TrendMicroDSA', 'Antimalware', 'EndpointProtection','SCWPAgent', 'PortalProtectExtension', 'FileSecurity', 'IaaSAntimalware'])\n",
    "    for vm in virtual_machines:\n",
    "        name = vm['name']\n",
    "        resource_group = vm['resourceGroup']\n",
    "#         endpoint_protection = !az vm show --resource-group {resource_group} --name {name} -d\n",
    "#         endpoint_protection = yaml.load(endpoint_protection.nlstr)\n",
    "        extensions = !az vm extension list --vm-name {name} --resource-group {resource_group}\n",
    "        extensions = yaml.load(extensions.nlstr)\n",
    "        has_protection = False\n",
    "        for extension in extensions:\n",
    "            if set([extension['virtualMachineExtensionType']]).intersection(accepted_protections):\n",
    "                has_protection = True\n",
    "        if not has_protection:\n",
    "            items_flagged_list.append((resource_group, name, extension.get('virtualMachineExtensionType', \"No extension\")))\n",
    "\n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': len(virtual_machines)}\n",
    "    metadata = {\"finding_name\": \"endpoint_protection_for_all_virtual_machines_is_installed\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Resource Group\", \"Name\", \"Unapproved Extension\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "\n",
    "def test_controls():\n",
    "    results = {}\n",
    "    virtual_machines = load_virtual_machines(virtual_machines_path)\n",
    "    results['vm_agent_is_installed'] = vm_agent_is_installed_7_1(virtual_machines)\n",
    "    results['os_disk_is_encrypted'] = os_disk_is_encrypted_7_2(virtual_machines)\n",
    "    results['data_disks_are_encrypted'] = data_disks_are_encrypted_7_3(virtual_machines)\n",
    "    results['only_approved_extensions_are_installed'] = only_approved_extensions_are_installed_7_4(virtual_machines)\n",
    "    results['endpoint_protection_for_all_virtual_machines_is_installed'] = endpoint_protection_for_all_virtual_machines_is_installed_7_6(virtual_machines)\n",
    "    \n",
    "    with open(filtered_virtual_machines_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, sort_keys=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Security Considerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvaults_path = os.path.join(raw_data_dir, 'keyvaults.json')\n",
    "keyvault_keys_and_secrets_metadata_path = os.path.join(raw_data_dir, 'keyvault_keys_and_secrets_metadata.json')\n",
    "locked_resources_path = os.path.join(raw_data_dir, 'locked_resources.json')\n",
    "other_security_considerations_filtered_path = os.path.join(filtered_data_dir, 'other_security_considerations_filtered.json')\n",
    "\n",
    "def get_keyvaults(keyvaults_path):\n",
    "    \"\"\"\n",
    "    @keyvaults_path: string - path to output json file\n",
    "    @returns: list of virtual_machines dict\n",
    "    \"\"\"\n",
    "    keyvaults = !az keyvault list\n",
    "    keyvaults = yaml.load(keyvaults.nlstr)\n",
    "    with open(keyvaults_path, 'w') as f:\n",
    "        json.dump(keyvaults, f, indent=4, sort_keys=True)\n",
    "    return keyvaults\n",
    "\n",
    "def load_keyvaults(keyvaults_path):\n",
    "    with open(keyvaults_path, 'r') as f:\n",
    "        keyvaults = yaml.load(f)\n",
    "    return keyvaults\n",
    "\n",
    "def get_locked_resources():\n",
    "    lock_list = !az lock list\n",
    "    lock_list = yaml.load(lock_list.nlstr)\n",
    "\n",
    "    with open(locked_resources_path, 'w') as f:\n",
    "        json.dump(lock_list, f, indent=4, sort_keys=True)\n",
    "    return lock_list\n",
    "\n",
    "def load_locked_resources(locked_resources_path):\n",
    "    with open(locked_resources_path, 'r') as f:\n",
    "        locked_list = yaml.load(f)\n",
    "    return locked_list\n",
    "\n",
    "def get_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path, keyvaults):\n",
    "    metadata = {}\n",
    "    for keyvault in keyvaults:        \n",
    "        vault_name = keyvault['name']\n",
    "        metadata[vault_name] = {}\n",
    "        keys = !az keyvault key list --vault-name {vault_name}\n",
    "        keys = yaml.load(keys.nlstr)\n",
    "        metadata[vault_name]['keys'] = keys\n",
    "        secrets = !az keyvault secret list --vault-name {vault_name}\n",
    "        secrets = yaml.load(secrets.nlstr)\n",
    "        metadata[vault_name]['secrets'] = secrets\n",
    "    \n",
    "    with open(keyvault_keys_and_secrets_metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4, sort_keys=True)\n",
    "    return metadata\n",
    "\n",
    "def load_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path):\n",
    "    with open(keyvault_keys_and_secrets_metadata_path, 'r') as f:\n",
    "        metadata = yaml.load(f)\n",
    "    return metadata\n",
    "\n",
    "def get_data():\n",
    "    keyvaults = get_keyvaults(keyvaults_path)\n",
    "    get_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path, keyvaults)\n",
    "    get_locked_resources()\n",
    "\n",
    "MAX_EXPIRY_ROTATION_DAYS = 730\n",
    "# 8.1 and 8.2\n",
    "def expiry_date_is_set_on_all_keys_and_secrets(keyvault_keys_and_secrets_metadata):\n",
    "    items_flagged_list = []\n",
    "    items_checked = 0\n",
    "    today = datetime.datetime.today()\n",
    "    \n",
    "    def get_key_or_secret_status(info):\n",
    "        enabled = info['attributes']['enabled']\n",
    "        created = datetime.datetime.strptime(info['attributes']['created'].split('T')[0], '%Y-%m-%d')\n",
    "        expires = info['attributes']['expires']\n",
    "        status = \"ok\"\n",
    "        if expires:\n",
    "            expires = datetime.datetime.strptime(expires.split('T')[0], '%Y-%m-%d')\n",
    "            expiry_delta = expires - created\n",
    "            if today > expires:\n",
    "                satus = \"expired\"\n",
    "            elif expiry_delta > datetime.timedelta(days=MAX_EXPIRY_ROTATION_DAYS):\n",
    "                status = \"exceeds max expiry days\"\n",
    "            # convert times back to a string for display\n",
    "            expires = expires.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            status = \"no expiry\"\n",
    "            expires = \"None\"\n",
    "        created = created.strftime('%Y-%m-%d')            \n",
    "            \n",
    "        return status, created, expires\n",
    "    \n",
    "    for keyvault_name, metadata in keyvault_keys_and_secrets_metadata.items():\n",
    "        for key_info in metadata['keys']:\n",
    "            items_checked += 1\n",
    "            if \"ERROR\" in key_info:\n",
    "                print(\"ERROR\", metadata['keys'][\"ERROR\"])\n",
    "                items_flagged_list.append((keyvault_name, \"ACCESS_DENIED\", \"key\", \"N/A\", \"N/A\", \"N/A\"))\n",
    "                continue\n",
    "            key_name = key_info['kid'].split('/')[-1]\n",
    "            status, created, expires = get_key_or_secret_status(key_info)\n",
    "            if status != \"ok\":\n",
    "                items_flagged_list.append((keyvault_name, key_name, \"key\", status, created, expires))\n",
    "                \n",
    "        for secret_info in metadata['secrets']:\n",
    "            items_checked += 1\n",
    "            if \"ERROR\" in secret_info:\n",
    "                print(\"ERROR\",  metadata['secrets'][\"ERROR\"])\n",
    "                items_flagged_list.append((keyvault_name, \"ACCESS_DENIED\", \"secret\", \"N/A\", \"N/A\", \"N/A\"))\n",
    "                continue\n",
    "            secret_name = secret_info['id'].split('/')[-1]\n",
    "            status, created, expires = get_key_or_secret_status(secret_info)                \n",
    "            if status != \"ok\":\n",
    "                items_flagged_list.append((keyvault_name, secret_name, \"secret\", status, created, expires))\n",
    "    \n",
    "    stats = {'items_flagged': len(items_flagged_list),\n",
    "             'items_checked': items_checked}\n",
    "    metadata = {\"finding_name\": \"expiry_date_is_set_on_all_keys_and_secrets\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"KeyVault Name\", \"Name\", \"Type\", \"Status\", \"Created\", \"Expires\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "critical_resources_list = []\n",
    "def resource_locks_are_set_for_mission_critical_azure_resources_8_3(critical_resources_list, locked_resources):\n",
    "    \"\"\"\n",
    "    This finding needs some work.\n",
    "    It is not clear from `az lock list` what resource is being locked.\n",
    "    For now, best ignore comparison and flag an error if len(critical_resources_list) < len(locked_resources)\n",
    "    \"\"\"\n",
    "    items_flagged_list = []\n",
    "    critical_resources = set(critical_resources_list)\n",
    "    if len(locked_resources) == 0 and len(critical_resources_list) == 0:\n",
    "        stats = {'items_flagged': 1,\n",
    "                 'items_checked': 1}\n",
    "    else:\n",
    "        stats = {'items_flagged': len(critical_resources_list) - len(locked_resources),\n",
    "                 'items_checked': len(critical_resources_list)}\n",
    "        # This is really the inverse of the usual items flagged list\n",
    "        #items_flagged_list = list(critical_resources.intersection(set(locked_resources)))\n",
    "        items_flagged_list = [(x['name'], x['id'], x['notes']) for x in locked_resources]\n",
    "    metadata = {\"finding_name\": \"resource_locks_are_set_for_mission_critical_Azure_resources\",\n",
    "                \"negative_name\": \"\",\n",
    "                \"columns\": [\"Lock Name\", \"Lock ID\", \"Notes\"]}            \n",
    "    return  {\"items\": items_flagged_list, \"stats\": stats, \"metadata\": metadata}\n",
    "\n",
    "def test_controls():\n",
    "    keyvault_keys_and_secrets_metadata = load_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path)\n",
    "    locked_resources = load_locked_resources(locked_resources_path)\n",
    "    results = {}\n",
    "    results['expiry_date_is_set_on_all_keys_and_secrets'] = expiry_date_is_set_on_all_keys_and_secrets(keyvault_keys_and_secrets_metadata)\n",
    "    results['resource_locks_are_set_for_mission_critical_azure_resources'] = resource_locks_are_set_for_mission_critical_azure_resources_8_3(critical_resources_list, locked_resources)\n",
    "    \n",
    "    with open(other_security_considerations_filtered_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, sort_keys=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_controls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvaults = load_keyvaults(keyvaults_path)\n",
    "keyvault_keys_and_secrets_metadata = get_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path, keyvaults)\n",
    "keyvault_keys_and_secrets_metadata = load_keyvault_keys_and_secrets_metadata(keyvault_keys_and_secrets_metadata_path)\n",
    "keyvault_keys_and_secrets_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile render_utils.py\n",
    "import functools\n",
    "import yaml\n",
    "\n",
    "cis_scanner_root = '/praetorian-tools/azure_cis_scanner/'\n",
    "\n",
    "@functools.lru_cache(1, typed=False)\n",
    "def get_dirs(directory):\n",
    "    return [x for x in os.listdir(directory) if os.path.isdir(directory)]\n",
    "\n",
    "# figure out better way to get base dir or let user select in UI\n",
    "active_subscription_dir = get_dirs(scan_data_dir)[0]\n",
    "active_subscription_dir = subscription_dirname\n",
    "#active_subscription_dir = 'Development-6ff7f744'\n",
    "\n",
    "accounts = {}\n",
    "with open(os.path.join(base_dir, 'scans', 'accounts.json'), 'r') as f:\n",
    "    accounts = yaml.load(f)\n",
    "\n",
    "scans_root = os.path.join(scan_data_dir, active_subscription_dir)\n",
    "\n",
    "#APP_ROOT = os.path.dirname(os.path.abspath(__file__))\n",
    "APP_ROOT = os.path.join(cis_scanner_root, 'report')\n",
    "STATIC = os.path.join(APP_ROOT, 'static')\n",
    "\n",
    "with open(os.path.expanduser(APP_ROOT + '/cis_structure.yaml'), 'r') as f:\n",
    "    cis_structure = yaml.load(f)\n",
    "\n",
    "def set_scans_root(subscription_dirname=active_subscription_dir):\n",
    "    scans_root = os.path.join(base_dir, 'scans', subscription_dirname)\n",
    "    return scans_root\n",
    "\n",
    "@functools.lru_cache(maxsize=32, typed=False)\n",
    "def get_filtered_data_path(date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Get the filtered data root for the scan run on date=date or latest if date=None\n",
    "    Returns path, date where date is the most recent date with data <= requested date\n",
    "    \n",
    "    Directory structure is\n",
    "    <scans_root>/scans/<date>/<section_lowercase_underscores>.json\n",
    "    \"\"\"\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    if date:\n",
    "        if os.path.exists:\n",
    "            return os.path.join(scans_root, 'scans', date, 'filtered'), date\n",
    "        else:\n",
    "            raise ValueError(\"Filtered data requested for {} but file does not exist at {}\".format(\n",
    "                date, os.path.join(scans_root, 'scans')))\n",
    "    else:\n",
    "        dir_list = get_dirs(scans_root)\n",
    "        if len(dir_list) == 0:\n",
    "            print(\"No data found in {}.  Please run scanner first\".format(scans_root))\n",
    "        else:\n",
    "            date = sorted(dir_list)[0]\n",
    "            return os.path.join(scans_root, 'scans', date, 'filtered'), date\n",
    "\n",
    "@functools.lru_cache(maxsize=32, typed=False)\n",
    "def get_filtered_data(date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Returns a dict of filtered data for a specific date or latest (default)\n",
    "    \n",
    "    If a section is missing it will not be returned.\n",
    "    The structure is {\"Identity and Access Management\": {\"finding1\": results_dict1}, \"finding2\": results_dict2}\n",
    "    where results_dict has keys stats, metadata, items, date - where date is actual date where data was found\n",
    "    \"\"\"\n",
    "    subscription_dirname = set_scans_root(subscription_dirname)\n",
    "    print(\"subscription_dirname\", subscription_dirname)\n",
    "    filtered_data = {}\n",
    "    for section_name in cis_structure['section_ordering']:\n",
    "        section_data = get_filtered_data_by_section(section_name, subscription_dirname=subscription_dirname)\n",
    "        filtered_data[section_name] = section_data\n",
    "    return filtered_data\n",
    "\n",
    "@functools.lru_cache(maxsize=128, typed=False)\n",
    "def get_filtered_data_by_section(section_name, date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Get the latest data for a section returning first found <= date\n",
    "    @params sectoin_name: Name of CIS section as a string e.g. (\"Identity and Access Management\")\n",
    "    @params date: date in format 'YYYY-M-D', i.e. strftime(\"%Y-%m-%d\")\n",
    "    @returns filtered data, date\n",
    "    \"\"\"\n",
    "    # get date folders, most to least recent\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    dir_list = reversed(sorted(get_dirs(scans_root)))\n",
    "    section_name_file = '_'.join(map(str.lower, section_name.split(' '))) + '_filtered.json'\n",
    "    for dir_date in dir_list:\n",
    "        if date and (dir_date > date):\n",
    "            continue\n",
    "        filtered_data_path = os.path.join(scans_root, dir_date, 'filtered', section_name_file)\n",
    "        if os.path.exists(filtered_data_path):\n",
    "            with open(filtered_data_path, 'r') as f:\n",
    "                data = yaml.load(f)\n",
    "                data['date'] = dir_date\n",
    "                return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=1, typed=False)\n",
    "def get_latest_filtered_data(date=None, subscription_dirname=active_subscription_dir):\n",
    "    \"\"\"\n",
    "    Returns a dict as in get_filtered_data, but if a section is missing, it will search\n",
    "    back in time for a date where the section does exist.\n",
    "    \"\"\"\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    data = get_filtered_data(date)\n",
    "    if not data:\n",
    "        return None\n",
    "    else:\n",
    "        for section_name in cis_structure['section_ordering']:\n",
    "            if section_name not in data:\n",
    "                section_data = get_filtered_data_by_name(section_name, date, subscription_dirname=subsription_dirname)\n",
    "                if section_data:\n",
    "                    data[section_name] = section_data\n",
    "    return data\n",
    "\n",
    "@functools.lru_cache(maxsize=1, typed=False)\n",
    "def get_stats(subscription_dirname=active_subscription_dir):\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    stats = {}\n",
    "    dir_list = sorted(get_dirs(scans_root))\n",
    "    for section_name in cis_structure['section_ordering']:\n",
    "        stats[section_name] = {}\n",
    "        section_name_file = '_'.join(map(str.lower, section_name.split(' '))) + '_filtered.json'\n",
    "        for dir_date in dir_list:\n",
    "            filtered_data_path = os.path.join(scans_root, dir_date, 'filtered', section_name_file)\n",
    "            if os.path.exists(filtered_data_path):\n",
    "                with open(filtered_data_path, 'r') as f:\n",
    "                    data = yaml.load(f)\n",
    "                for finding_name, finding_data in data.items():\n",
    "                    if not finding_name in stats[section_name]:\n",
    "                        stats[section_name][finding_name] = {}\n",
    "                    stats[section_name][finding_name][dir_date] = finding_data['stats']\n",
    "    return stats\n",
    "\n",
    "@functools.lru_cache(maxsize=1, typed=False)\n",
    "def get_latest_stats(subscription_dirname=active_subscription_dir):\n",
    "    scans_root = set_scans_root(subscription_dirname)\n",
    "    latest_stats = {}\n",
    "    stats = get_stats()\n",
    "    for section_name in stats:\n",
    "        latest_stats[section_name] = {}\n",
    "        for finding_name in stats[section_name]:\n",
    "            date = max(stats[section_name][finding_name])\n",
    "            latest_stats[section_name][finding_name] = {\"date\": date, **stats[section_name][finding_name][date]}\n",
    "\n",
    "    return latest_stats\n",
    "\n",
    "def get_finding_name(finding_name, subsection_name):\n",
    "    \"\"\"\n",
    "    Get finding name from CIS_TOC using over-ride (finding_name) but defaulting to parsed subsection_name\n",
    "    \"\"\"\n",
    "    if finding_name:\n",
    "        return finding_name\n",
    "    else:\n",
    "        return underscore_name(subsection_name)\n",
    "\n",
    "def underscore_name(subsection_name):\n",
    "    return '_'.join(map(lambda x: x.lower(), subsection_name.split(' ')))\n",
    "\n",
    "def title_except(string):\n",
    "    articles = ['a', 'an', 'of', 'the', 'is', 'not', 'for', 'if']\n",
    "    word_list = re.split(' ', string)       # re.split behaves as expected\n",
    "    final = [word_list[0].capitalize()]\n",
    "    for word in word_list[1:]:\n",
    "        final.append(word if word in articles else word.capitalize())\n",
    "    return \" \".join(final)\n",
    "\n",
    "def get_finding_index(findings_list, finding):\n",
    "    for finding_entry in findings_list:\n",
    "        if finding_entry['subsection_name'] == finding:\n",
    "            return finding_entry\n",
    "    raise ValueError(\"finding {} not found in {}\".format(finding, findings_list))\n",
    "    \n",
    "# def get_summary_stats(section=None):\n",
    "# \t\"\"\"\n",
    "# \tReturn the sum over impacted_items in all findings in a section\n",
    "\n",
    "# \tIf section == None, sum over sections\n",
    "# \t\"\"\"\n",
    "# \tstats = get_stats()\n",
    "# \tif section:\n",
    "\n",
    "# \telse:\n",
    "# \t\tfor section in stats:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def find_finding_entry(section_toc, finding_underscore_name):\n",
    "    for finding_entry in section_toc:\n",
    "        if finding_underscore_name == get_finding_name(finding_entry['finding_name'], finding_entry['subsection_name']):\n",
    "            return finding_entry\n",
    "    return None\n",
    "\n",
    "def findings_summary(latex=False, subscription_dirname=subscription_dirname):\n",
    "    print('1', subscription_dirname)\n",
    "    data = get_filtered_data(subscription_dirname=subscription_dirname)\n",
    "    for section_name, section_findings in data.items():\n",
    "        if section_findings:\n",
    "            for finding_name, finding in section_findings.items():\n",
    "                if finding_name == 'date':\n",
    "                    print(\"Date is\", finding)\n",
    "                    continue\n",
    "                try:\n",
    "                    section_toc = cis_structure['TOC'][section_name]\n",
    "                    finding_entry = find_finding_entry(section_toc, finding_name)\n",
    "                    if finding_entry:\n",
    "                        title = finding_entry['subsection_name']\n",
    "                    else:\n",
    "                        title = ' '.join(finding_name.split('_'))\n",
    "                    if finding.get('stats') and (finding['stats']['items_flagged'] > 0):\n",
    "                        print(\"{}: {} - {} of {} failed\".format(section_name, finding_name, finding['stats']['items_flagged'], finding['stats']['items_checked']))\n",
    "                        if latex:\n",
    "                            render_latex(finding['items'], finding['metadata']['columns'], title_except(title))\n",
    "                        else:\n",
    "                            pprint(finding['items'])\n",
    "                        print('\\n')\n",
    "                    if finding_name in ['security_contact_phone_number_is_set', 'activity_log_alert_is_configured']:\n",
    "                        print(finding['items'])\n",
    "                except Exception as e:\n",
    "                    print(\"           finding\", finding.keys())\n",
    "                    print(e)\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import yaml\n",
    "    \n",
    "def clean_latex(tuple_entry):\n",
    "    \"\"\"\n",
    "    Filter/escape problematic characters\n",
    "    Our minerva pdf generator chokes on '_', '*', ...\n",
    "    and possibly other things.\n",
    "    \"\"\"\n",
    "    def _clean_latex(tuple_entry_string):\n",
    "        processed = False\n",
    "        for symbol in ['_', '*']:\n",
    "            if symbol in tuple_entry_string:\n",
    "                tuple_entry_string = tuple_entry_string.replace(symbol, '\\\\' + symbol)\n",
    "                processed = True\n",
    "        if processed:\n",
    "            return '\\\\texttt{' + tuple_entry_string + '}'\n",
    "        else:\n",
    "            return tuple_entry_string\n",
    "\n",
    "    return _clean_latex(str(tuple_entry))\n",
    "    \n",
    "    \n",
    "def render_latex(resource_tuples, header, title):\n",
    "    header = list(map(clean_latex, header))\n",
    "    title = clean_latex(title)\n",
    "    \n",
    "    render_table_start(header, title)\n",
    "    num_columns = len(header)\n",
    "    if num_columns > 1:\n",
    "        line = ' & '.join(['{}']*num_columns)\n",
    "    else:\n",
    "        line = '{}'\n",
    "    line = line + ' \\\\\\\\'\n",
    "    for resource_tuple in resource_tuples:\n",
    "        if type(resource_tuple) == str:\n",
    "            resource_tuple = (resource_tuple,)\n",
    "        print('    ' + line.format(*map(clean_latex, resource_tuple)))\n",
    "    render_table_end(header)\n",
    "    \n",
    "def render_table_start(header, title):\n",
    "    \"\"\"\n",
    "    Render latex table suitable for minerva rendering\n",
    "    \n",
    "            \n",
    "    If the elements in the table are very long you can correct spacing with invisible text like this:\n",
    "    {\\color[HTML]{FFFFFF} {}} & {\\color[HTML]{333333}{spacing}}{\\color[HTML]{FFFFFF} {}}{\\color[HTML]{333333}{spacing_2}} & {\\color[HTML]{333333}{spacing_3}}{\\color[HTML]{FFFFFF}{}} \\\\\n",
    "\n",
    "    TODO: Paginate pages to break nicely over many pages.\n",
    "    \n",
    "    Some exceptions used to manually fix for specific cases:\n",
    "    \n",
    "    Text wrap some long arrays based on answer by zyy on\n",
    "    https://tex.stackexchange.com/questions/54069/table-with-text-wrapping\n",
    "    \n",
    "    GROUPS column had variable length and this worked.\n",
    "    \\begin{tabular}{|l|>{\\centering\\arraybackslash}m{10cm}|}\n",
    "    \\hline\n",
    "    \\multicolumn{2}{|c|}{IAM Users without MFA} \\\\\n",
    "    \\rowcolor[HTML]{333333}\n",
    "    {\\color[HTML]{FFFFFF}USER NAME} & {\\color[HTML]{FFFFFF}GROUPS} \\\\\n",
    "    \"\"\"\n",
    "    num_columns = len(header)\n",
    "    entries = ['\\color[HTML]{FFFFFF}' + '{}'.format(clean_latex(x)) for x in header]\n",
    "    if num_columns > 1: \n",
    "        line = '} & {'.join(entries)\n",
    "    else:\n",
    "        line = entries[0]\n",
    "    columns_format = '{|' + '|'.join(['l']*num_columns) + '|}'\n",
    "    print('\\\\begin{tabular}' + '{}'.format(columns_format) + '\\n'\n",
    "         '    ' + '\\\\hline\\n' +\n",
    "         '    ' + '\\\\multicolumn{' + str(num_columns) + '}' +\n",
    "        '{|c|}' + '{' + title + '}' +  ' \\\\\\\\\\n' +\n",
    "        '    ' + '\\\\rowcolor[HTML]{333333}\\n' +\n",
    "        '    ' + '{' + line + '}' + ' \\\\\\\\' \n",
    "    )\n",
    "    \n",
    "\n",
    "def render_table_end(header):\n",
    "    \"\"\"\n",
    "    Render table end\n",
    "    \"\"\"\n",
    "    print('    \\\\hline\\n' +\n",
    "          '\\\\end{tabular}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_summary(latex=False, subscription_dirname=subscription_dirname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_summary(latex=True, subscription_dirname='Development-6ff7f744')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az monitor diagnostic-settings list --resource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"KeyVault\", \"Name\", \"Type\", \"Status\", \"Created\", \"Expires\"]\n",
    "render_latex(expiry_dates, header, \"Expiry Date is Set on All Keys and Secrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sql_results.keys():\n",
    "    print(key.split('_')[-1], key, len(sql_results[key]))\n",
    "    for resource_group, server_name, db_name in sql_results[key]:\n",
    "        print('{resource_group} & {server_name} & {db_name} \\\\\\\\'.format(resource_group=resource_group,\n",
    "             server_name=server_name,\n",
    "             db_name=db_name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_results(results, impacted_system_dict):\n",
    "    \"\"\"\n",
    "    Wraps functions that return a tuple (passed(bool), impacted_closure(OrderedDict), result(json object - dict, list, ...)\n",
    "    @results: object reperesenting results dict of {func.__name__'s: [(impacted_system_dict + impacted_closure, result)]\n",
    "              impacted_system_tyuple is know prior to query\n",
    "              impacted_closure is the final identifying component(s) such as {db: db1} when (resource_group, sql_server) are given\n",
    "                  $ az sql db list --resource-group $resource_group --server $server_name\n",
    "\n",
    "    @impacted_systems_dict: OrderedDict([(key1, val1), (key2, val2),]) identifying (possibly) failing resource\n",
    "                 eg. {resource_group: rg-val, server: server-val} for sql tests\n",
    "                 or {resource_group: rg-val, storage_account: storage-val, disk: disk-val) for storage\n",
    "                 impacted_system_tuple is often required in the query:\n",
    "    @returns: (bool) passed, (OrderedDict) impacted_system, (json object) result\n",
    "              where impacted_system = OrderedDict(list(impacted_system_dict.items()) + list(impacted_closure.items())) \n",
    "    \"\"\"\n",
    "    def decorate(func):\n",
    "        def call(*args, **kwargs):\n",
    "            # *impacted_closure matches anything between first and last in func's return tuple if it exists\n",
    "            passed, *impacted_closure, result = func(*args, **kwargs)\n",
    "            impacted_system = OrderedDict(list(impacted_system_dict.items()) + list(impacted_closure.items()))\n",
    "            return apply_gen_findings(func, passed, impacted_system, result, results, *args, **kwargs)\n",
    "        return call\n",
    "    return decorate\n",
    "\n",
    "def apply_gen_results(func, passed, impacted_system, result, results, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    @func: function to wrap\n",
    "    @passed: bool - True of passed, False if failed\n",
    "    @impacted_system: OrderedDict(list(impacted_system_dict.items()) + list(impacted_closure.items())) \n",
    "    @result: object with important output data from test\n",
    "    @results: dict with keys func.__name__ and values list of dicts with keys impacted_system_tuple, value data\n",
    "    \"\"\"\n",
    "    if not passed:\n",
    "        pairs = results.get(func.__name__, [])\n",
    "        pairs.append((impacted_system, result))\n",
    "        results[func.__name__] = pairs\n",
    "    return passed, impacted_system, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(\"expiry_date_is_set_on_all_keys_and_secrets\".split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_path = '/praetorian-tools/azure_cis_scanner/azure_cis_scanner/modules'\n",
    "for filename in os.listdir(modules_path)[0:1]:\n",
    "    path = os.path.join(modules_path, filename)\n",
    "    print(path)\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if ('!az' in line) or ('!curl' in line):\n",
    "                print(lines[i].replace('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipycall(cmd):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = '    lock_list = !az lock list\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indent(line):\n",
    "    count = 0\n",
    "    for char in line:\n",
    "        if ord(line[0]) == 32:\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indent(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install msrestazure azure-cli-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.common.client_factory import get_client_from_cli_profile, get_client_from_auth_file\n",
    "from azure.mgmt.compute import ComputeManagementClient\n",
    "from azure.mgmt.resource import ResourceManagementClient, SubscriptionClient\n",
    "from msrestazure.azure_active_directory import MSIAuthentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client_from_cli_profile(SubscriptionClient)\n",
    "#print(dir(client.config))\n",
    "subscription_accounts = [ x.as_dict() for x in list(client.subscriptions.list())]\n",
    "print(subscription_accounts)\n",
    "\n",
    "#dprint(list(map(dict, list(client.subscriptions.list()))))\n",
    "#dprint(client.subscriptions.get()['display_name'])\n",
    "\n",
    "# credentials = MSIAuthentication()\n",
    "# # Create a Subscription Client\n",
    "# subscription_client = SubscriptionClient(credentials)\n",
    "# subscription = next(subscription_client.subscriptions.list())\n",
    "# subscription_id = subscription.subscription_id\n",
    "# print(subscription_id)\n",
    "# # Create a Resource Management client\n",
    "# resource_client = ResourceManagementClient(credentials, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azurerm\n",
    "import yaml\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat '/root/.azure/azureProfile.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/.azure/accessTokens.json', 'r') as f:\n",
    "    data = yaml.load(f)\n",
    "print(data[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['tokenType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subscriptions(client):\n",
    "    return [x.as_dict() for x in client.subscriptions.list().advance_page()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptions = get_subscriptions(client)\n",
    "subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "import os.path\n",
    "\n",
    "def get_cli_profile():\n",
    "    \"\"\"Return a CLI profile class.\n",
    "\n",
    "    .. versionadded:: 1.1.6\n",
    "\n",
    "    :return: A CLI Profile\n",
    "    :rtype: azure.cli.core._profile.Profile\n",
    "    :raises: ImportError if azure-cli-core package is not available\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        from azure.cli.core._profile import Profile\n",
    "        from azure.cli.core._session import ACCOUNT\n",
    "        from azure.cli.core._environment import get_config_dir\n",
    "    except ImportError:\n",
    "        raise ImportError(\"You need to install 'azure-cli-core' to load CLI credentials\")\n",
    "\n",
    "\n",
    "    azure_folder = get_config_dir()\n",
    "    ACCOUNT.load(os.path.join(azure_folder, 'azureProfile.json'))\n",
    "    return Profile(storage=ACCOUNT)\n",
    "\n",
    "def get_azure_cli_credentials(resource=None, with_tenant=False, subscription_id=None):\n",
    "    \"\"\"Return Credentials and default SubscriptionID of current loaded profile of the CLI.\n",
    "\n",
    "    Credentials will be the \"az login\" command:\n",
    "    https://docs.microsoft.com/cli/azure/authenticate-azure-cli\n",
    "\n",
    "    Default subscription ID is either the only one you have, or you can define it:\n",
    "    https://docs.microsoft.com/cli/azure/manage-azure-subscriptions-azure-cli\n",
    "\n",
    "    .. versionadded:: 1.1.6\n",
    "\n",
    "    :param str resource: The alternative resource for credentials if not ARM (GraphRBac, etc.)\n",
    "    :param bool with_tenant: If True, return a three-tuple with last as tenant ID\n",
    "    :return: tuple of Credentials and SubscriptionID (and tenant ID if with_tenant)\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    profile = get_cli_profile()\n",
    "    # if subscription_id:\n",
    "    #     profile.set_active_subscription(subscription_id)\n",
    "        \n",
    "    cred, subscription_id, tenant_id = profile.get_login_credentials(resource=resource, subscription_id=subscription_id)\n",
    "    #cred, subscription_id, tenant_id = profile.get_login_credentials(resource=resource)\n",
    "    if with_tenant:\n",
    "        return cred, subscription_id, tenant_id\n",
    "    else:\n",
    "        return cred, subscription_id\n",
    "\n",
    "try:\n",
    "    from msrest.authentication import (\n",
    "        BasicAuthentication,\n",
    "        BasicTokenAuthentication,\n",
    "        OAuthTokenAuthentication\n",
    "    )\n",
    "except ImportError:\n",
    "    raise ImportError(\"You need to install 'msrest' to use this feature\")\n",
    "\n",
    "try:\n",
    "    from msrestazure.azure_active_directory import (\n",
    "        InteractiveCredentials,\n",
    "        ServicePrincipalCredentials,\n",
    "        UserPassCredentials\n",
    "    )\n",
    "except ImportError:\n",
    "    raise ImportError(\"You need to install 'msrestazure' to use this feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.common.client_factory import get_client_from_cli_profile, get_client_from_auth_file\n",
    "from azure.mgmt.compute import ComputeManagementClient\n",
    "from azure.mgmt.advisor import AdvisorManagementClient\n",
    "from azure.mgmt.sql import SqlManagementClient\n",
    "from azure.mgmt.monitor import MonitorManagementClient\n",
    "from azure.mgmt.network import NetworkManagementClient\n",
    "from azure.mgmt.monitor.models import RuleMetricDataSource\n",
    "from azure.mgmt.keyvault import KeyVaultManagementClient\n",
    "from azure.keyvault import KeyVaultClient\n",
    "from azure.mgmt.resource import ResourceManagementClient, SubscriptionClient\n",
    "from azure.common.credentials import ServicePrincipalCredentials\n",
    "\n",
    "AZURE_CONFIG_DIR = os.path.expanduser('~/.azure')\n",
    "AZURE_PROFILE_PATH = os.path.join(AZURE_CONFIG_DIR, 'azureProfile.json')\n",
    "AZURE_CREDENTIALS_PATH = os.path.join(AZURE_CONFIG_DIR, 'credentials')\n",
    "\n",
    "\n",
    "def get_clients(tenant_id=None, subscription_id=None, generate_credentials_ini=False, generate_auth_file=False, overwrite_ini=False, use_service_principals=False):\n",
    "    \"\"\"\n",
    "    Create a client for each of the subscriptions in azureProfile.json for a tenant\n",
    "    \"\"\"\n",
    "    credentials_ini = \"\"\n",
    "\n",
    "    with open(AZURE_PROFILE_PATH, 'r') as f:\n",
    "        azure_profiles = yaml.load(f)['subscriptions']\n",
    "    for profile in azure_profiles:\n",
    "        if tenant_id and not (tenant_id == profile['tenantId']):\n",
    "            continue\n",
    "        if subscription_id and not (subscription_id == profile['id']):\n",
    "            continue\n",
    "        subscription_id = profile['id']\n",
    "        subscription_name = profile['name']\n",
    "        service_principle_name = profile['name'] + '-' + subscription_id\n",
    "        \n",
    "        credentials = get_azure_cli_credentials(resource=None, with_tenant=False, subscription_id=subscription_id)[0]\n",
    "        print(dir(credentials))\n",
    "        print(credentials.signed_session())\n",
    "        if use_service_principals:\n",
    "            result = call(\"az account set --subscription {}\".format(subscription_id))\n",
    "            print(result)\n",
    "            credentials = json.loads(call(\"az ad sp create-for-rbac --sdk-auth\", stderr=None))\n",
    "            sp_credentials = ServicePrincipalCredentials(\n",
    "            client_id=credentials['clientId'],\n",
    "            secret=credentials['clientSecret'],\n",
    "            tenant=credentials['tenantId'])\n",
    "        \n",
    "        print(credentials)\n",
    "\n",
    "        if generate_credentials_ini:\n",
    "            credentials_ini += credentials_block(subscription_id, credentials, service_principle_name)\n",
    "        if generate_auth_file:\n",
    "            azure_auth_location = os.path.join(AZURE_CONFIG_DIR, service_principle_name + '.json')\n",
    "            with open(azure_auth_location, 'w') as f:\n",
    "                f.write(credentials)\n",
    "\n",
    "\n",
    "        if generate_credentials_ini:\n",
    "            if overwrite_ini:\n",
    "                mode = 'w'\n",
    "            else:\n",
    "                mode = 'w+'\n",
    "            with open(AZURE_CREDENTIALS_PATH, mode) as f:\n",
    "                f.write(credentials_ini)\n",
    "\n",
    "        print(\"creating subscription client\")\n",
    "        subscription_client = SubscriptionClient(sp_credentials)\n",
    "        print(\"creating compute client\")\n",
    "        compute_credentials = get_azure_cli_credentials(resource=\"Compute\", with_tenant=False, subscription_id=subscription_id)[0]\n",
    "        compute_client = ComputeManagementClient(credentials, subscription_id)\n",
    "        print(\"creating sql client\")\n",
    "        sql_client = SqlManagementClient(sp_credentials, subscription_client)\n",
    "        print(\"creating keyvault management client\")\n",
    "        kvm_client = KeyVaultManagementClient(credentials, subscription_id)        \n",
    "        print(\"creating keyvault client\")\n",
    "        print('type', type(sp_credentials))\n",
    "        kv_client = KeyVaultClient(sp_credentials)       \n",
    "        print(\"creating monitoring client\")\n",
    "        monitoring_client = MonitorManagementClient(credentials, subscription_id)\n",
    "        network_client = NetworkManagementClient(credentials, 'your-subscription-id')\n",
    "        advisor_client = AdvisorManagementClient(sp_credentials, subscription_id)\n",
    "        if use_service_principals:\n",
    "            yield subscription_client, compute_client, sql_client, kvm_client, kv_client, monitoring_client, advisor_client\n",
    "        else:\n",
    "            yield subscription_client, compute_client, sql_client, kvm_client, monitoring_client, advisor_client\n",
    "\n",
    "def credentials_block(subscription_id, credentials, service_principal_name, is_default=False):\n",
    "    \"\"\"\n",
    "    Generate a block for use in ~/.azure/credentials ini file\n",
    "    \"\"\"\n",
    "    client_id = credentials['clientId']\n",
    "    secret = credentials['clientSecret']\n",
    "    tenant_id = credentials['tenantId']\n",
    "\n",
    "    ini_content = \"\"\n",
    "\n",
    "    if is_default:\n",
    "        ini_content += \"\"\" \n",
    "[default]\n",
    "subscription_id={}\n",
    "client_id={}\n",
    "secret={}\n",
    "tenant={}\n",
    "\"\"\".format(subscription_id, client_id, secret, tenant_id)\n",
    "\n",
    "    else:\n",
    "        ini_content += \"\"\" \n",
    "[{}]\n",
    "subscription_id={}\n",
    "client_id={}\n",
    "secret={}\n",
    "tenant={}\n",
    "\"\"\".format(service_principal_name, subscription_id, client_id, secret, tenant_id)\n",
    "    return ini_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials_from_cli(tenant_id=None, subscription_id=None):\n",
    "    \"\"\"\n",
    "    Create a credential for each of the subscriptions in azureProfile.json for a tenant\n",
    "    @param tenant_id: uuid string - if None, iterate over all tenant_ids\n",
    "    @param subscription_id: uuid string - if None iterate over all subscription_ids\n",
    "    @returns: list of (tenant_id, subscription_id, subscription_name, credentials) where credentials is an ADAL signed session \n",
    "              bound to the tenant_id and subscription\n",
    "    \"\"\"\n",
    "\n",
    "    with open(AZURE_PROFILE_PATH, 'r') as f:\n",
    "        azure_profiles = yaml.load(f)['subscriptions']\n",
    "    results = []\n",
    "    for profile in azure_profiles:\n",
    "        if tenant_id and not (tenant_id == profile['tenantId']):\n",
    "            continue\n",
    "        tenant_id = profile['tenantId']\n",
    "        if subscription_id and (subscription_id != profile['id']):\n",
    "            continue\n",
    "        subscription_id = profile['id']\n",
    "        subscription_name = profile['name']\n",
    "        service_principle_name = profile['name'] + '-' + subscription_id\n",
    "        \n",
    "        # this is a modification of https://github.com/Azure/azure-sdk-for-python/blob/master/azure-common/azure/common/credentials.py\n",
    "        # until https://github.com/Azure/azure-sdk-for-python/issues/2898 gets fixed\n",
    "        print('get_clients_from_cli', subscription_id, tenant_id)\n",
    "        credentials = get_azure_cli_credentials(resource=None, with_tenant=False, subscription_id=subscription_id)[0]\n",
    "        \n",
    "        results.append((tenant_id, subscription_id, subscription_name, credentials))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant_id, subscription_id, _, creds = get_credentials_from_cli()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disks = sp_compute_client.disks.list()\n",
    "snaps = sp_compute_client.snapshots.list()\n",
    "#vms = sp_compute_client.virtual_machines.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_credentials = get_azure_cli_credentials(resource=\"Compute\", with_tenant=False, subscription_id='510f92e0-3fcf-4b8f-8a23-095d37e6a299')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_client.virtual_machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_client.database_threat_detection_policies.get('scannerTestSqlRg', 'kv1208604109466264035', 'master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_client.server_azure_ad_administrators.get('scannerTestSqlRg', 'kv1208604109466264035', 'master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_compute_client = ComputeManagementClient(sp_credentials, '510f92e0-3fcf-4b8f-8a23-095d37e6a299')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_compute_client.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_credentials.construct_auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_compute_client.config.credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_client._client.config.credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_from_paged_results(snaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_from_paged_results(disks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call(\"az account set --subscription {}\".format(subscription_id))\n",
    "print(result)\n",
    "credentials = json.loads(call(\"az ad sp create-for-rbac --sdk-auth\", stderr=None))\n",
    "sp_credentials = ServicePrincipalCredentials(\n",
    "client_id=credentials['clientId'],\n",
    "secret=credentials['clientSecret'],\n",
    "tenant=credentials['tenantId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "rand_service_principal_name = \"az_scanner-\" + secrets.token_hex(8)\n",
    "rand_password = \"az_scanner_secret-\" + secrets.token_hex(8)\n",
    "credentials = json.loads(call(\"az ad sp create-for-rbac --name {rand_service_principal_name} --password {rand_password}\".format(rand_service_principal_name, rand_password), stderr=None))\n",
    "sp_credentials = ServicePrincipalCredentials(\n",
    "client_id=credentials['clientId'],\n",
    "secret=credentials['clientSecret'],\n",
    "tenant=credentials['tenantId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_credentials.construct_auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subscription_client, compute_client, sql_client, kvm_client, kv_client, monitoring_client in get_clients in get_clients():\n",
    "#     print(subscription_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_generator = get_clients(tenant_id='06fbf4a8-34be-4de2-800d-b60b4b8e4610', subscription_id='510f92e0-3fcf-4b8f-8a23-095d37e6a299', use_service_principals=True)\n",
    "subscription_client, compute_client, sql_client, kvm_client, kv_client, monitoring_client, advisor_client = next(clients_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_from_paged_results(subscription_client.subscriptions.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_servers = get_list_from_paged_results(sql_client.servers.list(subscription_id='510f92e0-3fcf-4b8f-8a23-095d37e6a299'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_client.database_threat_detection_policies.get('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = advisor_client.recommendations.list()\n",
    "get_list_from_paged_results(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advisor_client.recommendations.get('/subscriptions/510f92e0-3fcf-4b8f-8a23-095d37e6a299/providers/Microsoft.Advisor/recommendations/2982b036-e229-b557-5fb4-426675afdde0', '2982b036-e229-b557-5fb4-426675afdde0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_from_paged_results(advisor_client.operations.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advisor_client.recommendations.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advisor_client.recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_url = '/subscriptions/510f92e0-3fcf-4b8f-8a23-095d37e6a299/resourceGroups/kb-may2018-lab/providers/Microsoft.KeyVault/vaults/kbroughton-keyvault-1'\n",
    "vault_base_url = 'https://kbroughton-keyvault-1.vault.azure.net'\n",
    "#kv_url = os.path.join(vault_base_url, kv_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_client.config.base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvm_client.config.credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyVaultClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kv_client.config.base_url = 'https://management.azure.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_client.config.base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_keys = kv_client.get_keys(vault_base_url)\n",
    "kv_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_from_paged_results(kv_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvm_client.vaults.create_or_update(group_name, vault_name, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_keys.get(kv_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible path traversal?\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "<ipython-input-212-d2c3c2f74da1> in <module>()\n",
    "----> 1 kv_keys.get(kv_url)\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/msrest/paging.py in get(self, url)\n",
    "     93         self.reset()\n",
    "     94         self.next_link = url\n",
    "---> 95         return self.advance_page()\n",
    "     96 \n",
    "     97     def reset(self):\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/msrest/paging.py in advance_page(self)\n",
    "    115             raise StopIteration(\"End of paging\")\n",
    "    116         self._current_page_iter_index = 0\n",
    "--> 117         self._response = self._get_next(self.next_link)\n",
    "    118         self._derserializer(self, self._response)\n",
    "    119         return self.current_page\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/azure/keyvault/v7_0/key_vault_client.py in internal_paging(next_link, raw)\n",
    "    599 \n",
    "    600             # Construct and send request\n",
    "--> 601             request = self._client.get(url, query_parameters)\n",
    "    602             response = self._client.send(\n",
    "    603                 request, header_parameters, stream=False, **operation_config)\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/msrest/service_client.py in get(self, url, params, headers, content, form_content)\n",
    "    456         :param dict form_content: Form content\n",
    "    457         \"\"\"\n",
    "--> 458         request = self._request(url, params, headers, content, form_content)\n",
    "    459         request.method = 'GET'\n",
    "    460         return request\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/msrest/service_client.py in _request(self, url, params, headers, content, form_content)\n",
    "    249 \n",
    "    250         if url:\n",
    "--> 251             request.url = self.format_url(url)\n",
    "    252 \n",
    "    253         if params:\n",
    "\n",
    "/usr/local/lib/python3.5/dist-packages/msrest/service_client.py in format_url(self, url, **kwargs)\n",
    "    428         if not parsed.scheme or not parsed.netloc:\n",
    "    429             url = url.lstrip('/')\n",
    "--> 430             base = self.config.base_url.format(**kwargs).rstrip('/')\n",
    "    431             url = urljoin(base + '/', url)\n",
    "    432         return url\n",
    "\n",
    "KeyError: 'vaultBaseUrl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Possible path traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_from_paged_results(kv_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_client, compute_client, sql_client, kv_client, monitoring_client = next(clients_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results = kv_client.vaults.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results.raw.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_from_paged_results(paged):\n",
    "    results = []\n",
    "    results.extend(paged.advance_page())\n",
    "    while paged.next_link:\n",
    "        results.extend(paged.advance_page())\n",
    "    return [x.as_dict() for x in results]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_from_paged_results(paged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(kv_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Azure/azure-sdk-for-python/blob/master/doc/sample_azure-keyvault.rst\n",
    "# https://azure.microsoft.com/en-us/resources/samples/?platform=python&term=key+vault&sort=0\n",
    "kv_client.vaults.get.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(kv_client.vaults.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account set --subscription 6ff7f744-b7aa-4894-94b3-0ed92b8b2866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '510f92e0-3fcf-4b8f-8a23-095d37e6a299'\n",
    "def get_resource_groups(client, subscription_id):\n",
    "    groups = []\n",
    "    rm = ResourceManagementClient(client.config.credentials, subscription_id)\n",
    "    rgs = rm.resource_groups.list()\n",
    "    groups.extend(rgs.advance_page())\n",
    "    while rgs.next_link:\n",
    "        groups.extend(rgs.advance_page())\n",
    "    return [x.as_dict() for x in groups]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_resource_groups(client, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyvaults():\n",
    "    pass\n",
    "# https://azure.microsoft.com/en-us/resources/samples/key-vault-python-network-acl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_logs():\n",
    "    today = datetime.datetime.now().date()\n",
    "    filter = \" and \".join([\n",
    "        \"eventTimestamp ge {}\".format(today),\n",
    "        \"resourceGroupName eq 'ResourceGroupName'\"\n",
    "    ])\n",
    "    select = \",\".join([\n",
    "        \"eventName\",\n",
    "        \"operationName\"\n",
    "    ])\n",
    "\n",
    "    activity_logs = client.activity_logs.list(\n",
    "        filter=filter,\n",
    "        select=select\n",
    "    )\n",
    "    for log in activity_logs:\n",
    "        # assert isinstance(log, azure.monitor.models.EventData)\n",
    "        print(\" \".join([\n",
    "            log.event_name.localized_value,\n",
    "            log.operation_name.localized_value\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vms(client, subscription_id):\n",
    "    instances = []\n",
    "    resource_groups = get_resource_groups(client, subscription_id)\n",
    "    compute = ComputeManagementClient(client.config.credentials, subscription_id)\n",
    "    print(resource_groups)\n",
    "    for resource_group in [x['name'] for x in resource_groups]:\n",
    "        vms = compute.virtual_machines.list(resource_group)\n",
    "        print(vms)\n",
    "        instances.extend(vms.advance_page())\n",
    "    return [x.as_dict() for x in instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vms = get_vms(client, subscription_id)\n",
    "vms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph API\n",
    "These looked promising, but don't seem to have the answers for IAM section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.graphrbac import GraphRbacManagementClient\n",
    "import jmespath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# today = datetime.datetime.now().date()\n",
    "# _filter = \" and \".join([ \"eventTimestamp ge '{}T00:00:00Z'\".format(today), \"resourceGroupName eq 'jay'\" ])\n",
    "\n",
    "credentials = ServicePrincipalCredentials(client_id=client_id, secret=secret, tenant=tenant)\n",
    "\n",
    "client = MonitorClient(credentials, subscription_id)\n",
    "select = \",\".join([ \"eventName\", \"operationName\" ])\n",
    "\n",
    "print(select)\n",
    "print(_filter)\n",
    "activity_logs = client.activity_logs.list( filter=filter, select=select )\n",
    "\n",
    "# for log in activity_logs:\n",
    "#     # assert isinstance(log, azure.monitor.models.EventData)\n",
    "#     print(\" \".join([\n",
    "#         log.event_name.localized_value,\n",
    "#         log.operation_name.localized_value\n",
    "#     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/lib/python3.5/dist-packages/azure/mgmt/sql/sql_management_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /usr/local/lib/python3.5/dist-packages/azure/mgmt/sql/sql_management_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /usr/local/lib/python3.5/dist-packages/azure/graphrbac/version.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbac = GraphRbacManagementClient(client.config.credentials, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbac.users.list().advance_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsess = rbac.config.credentials.signed_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsess.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = ServicePrincipalCredentials(\n",
    "    #client_id=CLIENT_ID,\n",
    "    #secret=SECRET,\n",
    "    #tenant=TENANT_ID,\n",
    "    resource=\"https://graph.windows.net\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.as_dict() for x in client.tenants.list().advance_page()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant_id = jmespath.search('[?id == `{}`].tenantId | [0]'.format(subscription_id), accounts)\n",
    "tenant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gclient = get_client_from_cli_profile(GraphRbacManagementClient, tenant_id=tenant_id)\n",
    "get_list_from_paged_results(gclient.service_principals.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gclient.service_principals.config.credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(gclient):\n",
    "    return [x.as_dict() for x in gclient.groups.list().advance_page()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_groups(gclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 ensure there are no guest users\n",
    "def get_users(gclient):\n",
    "    return [x.as_dict() for x in gclient.users.list().advance_page()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_users(gclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains(gclient):\n",
    "    return [x.as_dict() for x in gclient.domains.list().advance_page()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_domains(gclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objects(gclient):\n",
    "    return [x.as_dict() for x in gclient.objects.get_objects_by_object_ids(parameters={\"include_directory_object_references\": False,\n",
    "                                                                                      \"object_ids\": ['41a618bc-3a42-4b5d-ba82-4800b674621c']}).advance_page()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_principals(gclient):\n",
    "    return [x.as_dict() for x in gclient.service_principals.list().advance_page()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principals = get_service_principals(gclient)\n",
    "principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jmespath.search('[?appDisplayName == `Microsoft Graph`] | [0]', principals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jmespath.search('[?appDisplayName == `Microsoft Graph`] | [0]', principals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accountEnabled == True\n",
    "jmespath.search('[?appDisplayName == `Azure Multi-Factor Auth Client`] | [0]', principals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_cis_scanner/modules/other_security_considerations.py:    keyvaults = !az keyvault list\n",
    "azure_cis_scanner/modules/other_security_considerations.py:    lock_list = !az lock list\n",
    "azure_cis_scanner/modules/other_security_considerations.py:        keys = !az keyvault key list --vault-name {vault_name}\n",
    "azure_cis_scanner/modules/other_security_considerations.py:        secrets = !az keyvault secret list --vault-name {vault_name}\n",
    "azure_cis_scanner/modules/virtual_machines.py:    virtual_machines = !az vm list\n",
    "azure_cis_scanner/modules/virtual_machines.py:#         encrypted = !az vm encryption show --name {name} --resource-group {resource_group} --query dataDisk\n",
    "azure_cis_scanner/modules/virtual_machines.py:        extensions = !az vm extension list --vm-name {name} --resource-group {resource_group}\n",
    "azure_cis_scanner/modules/virtual_machines.py:#         endpoint_protection = !az vm show --resource-group {resource_group} --name {name} -d\n",
    "azure_cis_scanner/modules/virtual_machines.py:        extensions = !az vm extension list --vm-name {name} --resource-group {resource_group}\n",
    "azure_cis_scanner/modules/storage_accounts.py:        #activity_log = !az monitor activity-log list --resource-group {resource_group} --start-time {start_time}\n",
    "azure_cis_scanner/modules/storage_accounts.py:        #keys = !az storage account keys list --account-name {account_name} --resource-group {resource_group}\n",
    "azure_cis_scanner/modules/storage_accounts.py:        #container_list = !az storage container list --account-name {account_name} --account-key {account_key}\n",
    "azure_cis_scanner/modules/logging_and_monitoring.py:    keyvaults = !az keyvault list\n",
    "azure_cis_scanner/modules/logging_and_monitoring.py:        keyvault_settings = !az monitor diagnostic-settings list --resource {resource_id}\n",
    "azure_cis_scanner/modules/logging_and_monitoring.py:        monitor_diagnostic_settings = !az monitor diagnostic-settings list --resource {resource_id}\n",
    "azure_cis_scanner/modules/logging_and_monitoring.py:    monitor_log_profiles = !az monitor log-profiles list\n",
    "azure_cis_scanner/modules/logging_and_monitoring.py:        activity_log = !az monitor activity-log list --resource-group {resource_group} --start-time {start_time}\n",
    "azure_cis_scanner/modules/logging_and_monitoring.py:    activity_log_alerts = !az monitor activity-log alert list\n",
    "azure_cis_scanner/modules/networking.py:    network_security_groups = !az network nsg list\n",
    "azure_cis_scanner/modules/networking.py:    network_watcher = !az network watcher list\n",
    "azure_cis_scanner/modules/networking.py:        network_flow = !az network watcher flow-log show --resource-group {resource_group} --nsg {nsg_id}\n",
    "azure_cis_scanner/modules/sql_databases.py:    sql_servers = !az sql server list\n",
    "azure_cis_scanner/modules/sql_databases.py:        dbs = !az sql db list --resource-group $resource_group --server $server_name\n",
    "azure_cis_scanner/modules/sql_databases.py:            threat_policy = !az sql db threat-policy show --resource-group {resource_group} --server {server_name} --name {db_name}\n",
    "azure_cis_scanner/modules/sql_databases.py:            audit_policy = !az sql db audit-policy show --resource-group {resource_group} --server {server_name} --name {db_name}\n",
    "azure_cis_scanner/modules/sql_databases.py:            tde_policy = !az sql db tde show --resource-group {resource_group} --server {server_name} --database {db_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/engagements/vmware/repos/dumpster.json') as f:\n",
    "    dumpster = json.load(f)\n",
    "len(dumpster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpster[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_df = pd.DataFrame([(x['Details']['String'], x['File']) for x in dumpster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_df = dump_df.drop_duplicates()\n",
    "dump_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excludes = ['Compute_1', 'Compute_2', 'VmOne[0]', 'VmOne[1]', 'VmOne[2]', 'appOne[0]', 'appOne[1]', 'appOne[2]', 'Shape_1_', 'appTwo[0]', 'appTwo[1]', 'appTwo[2]', '[a-zA-Z0-9]*', 'Basic_A0']\n",
    "prefix_excludes = ['59', 'SHA', 'Deploy', 'User', 'Project', 'Akshata', 'tab', '#', ':', 'Standard', 'Plan', 'tag', 'Chrome', ',', 'Palette', '_', '=', 'Fn', 'Test_', 'E2E_', 'scale', '/some', 'some', 'Tango', '\\\\', 'target', 'source', 'form', '-Xm', 'src', 'Src', 'Windows', '[', 'Bat', '.', 'Std']\n",
    "def excluded(string, excludes, prefix_excludes):\n",
    "    if string in excludes:\n",
    "        return True\n",
    "    for prefix in prefix_excludes:\n",
    "        if string.startswith(prefix):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pwd_df = pd.DataFrame([(x['Details']['String'], x['File']) for x in dumpster if ( x['Finding'] == \"Password\") and not ( excluded(x['Details']['String'], excludes, prefix_excludes))], columns=['secret', 'file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pwd_df.loc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pwd_df.groupby(['secret']).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pwd_df.loc[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pwd_df['secret'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = ['VMware3!', 'VMware4!', 'VMware@123', 'VMware123!', '!321erawMV', 'VMware1!', 'Symphony1!', 'mathMl$1', 'Pa$$word1', 'Password1!' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = !ls /engagements/cis_test/scans/Pay-As-You-Go-510f92e0/\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reversed(sorted(dir_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
